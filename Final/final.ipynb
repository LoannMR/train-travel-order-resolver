{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "# load models\n",
    "model = Model(\"./models/vosk-model-fr-0.22\")\n",
    "recognizer = KaldiRecognizer(model, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "\n",
    "def start_voice_recognition():\n",
    "    \"\"\"\n",
    "    Speach to text.\n",
    "    Stop the voice recognition by saying \"stop\" or until a specific time\n",
    "    \"\"\"\n",
    "    mic = pyaudio.PyAudio()\n",
    "    stream = mic.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=8192)\n",
    "    stream.start_stream()\n",
    "\n",
    "    print(\"Listening...\")\n",
    "    while True:\n",
    "        data = stream.read(4096)\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            text = recognizer.Result()\n",
    "            text = text[14:-3]  # Get the sentence\n",
    "            print(text)\n",
    "            break  # Stop after the first sentence is recognized\n",
    "\n",
    "    # Close the stream and PyAudio instance\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    mic.terminate()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "alors comment l'état paraît aller au bout de quarante cinq à soixante\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"alors comment l'état paraît aller au bout de quarante cinq à soixante\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_voice_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Comment aller de paris à Lyon ?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import language_tool_python\n",
    "\n",
    "is_bad_rule = lambda rule: rule.message == 'Possible spelling mistake found.' and len(rule.replacements) and rule.replacements[0][0].isupper()\n",
    "with language_tool_python.LanguageToolPublicAPI('fr') as tool:\n",
    "    sentence = \"comment aller de paris a lyon\"\n",
    "    matches = tool.check(sentence)\n",
    "    corrected_sentence = tool.correct(sentence)\n",
    "    corrected_sentence = tool.correct(corrected_sentence)\n",
    "\n",
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "\n",
    "def spell_check(sentence):\n",
    "    \"\"\"\n",
    "    Correct spelling mistakes in the sentence as long as they occur\n",
    "    \"\"\"\n",
    "    # Initialize LanguageToolPublicAPI\n",
    "    with language_tool_python.LanguageToolPublicAPI('fr') as tool:\n",
    "        # Initialize the corrected version\n",
    "        corrected_old = sentence\n",
    "        corrected_new = tool.correct(corrected_old)\n",
    "\n",
    "        # Perform spell checking until no more corrections are made\n",
    "        while corrected_new != corrected_old and corrected_new:\n",
    "            corrected_old = corrected_new\n",
    "            corrected_new = tool.correct(corrected_old)\n",
    "\n",
    "    \n",
    "    return corrected_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Comment aller de Lyon à Marseille'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check(\"cmment aller de lyon a marseille\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mregex_replace(input_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^a-zA-Z0-9À-ÖØ-öø-ÿ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#Load the custom function (not seems to work)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# with open(\"./models/custom_standardization.pkl\", \"rb\") as file:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     custom_standardization = pickle.load(file) \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model_bi_gru \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/model_bidirectional_gru.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_standardization\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_standardization\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\loann\\.conda\\envs\\global\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\loann\\.conda\\envs\\global\\lib\\site-packages\\h5py\\_hl\\files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\loann\\.conda\\envs\\global\\lib\\site-packages\\h5py\\_hl\\files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#from keras.models import load_model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization(input_text):\n",
    "    # Remove punctuations, but preserve apostrophes\n",
    "    return tf.strings.regex_replace(input_text, \"[^a-zA-Z0-9À-ÖØ-öø-ÿ' ]\", \"\")\n",
    "\n",
    "#Load the custom function (not seems to work)\n",
    "# with open(\"./models/custom_standardization.pkl\", \"rb\") as file:\n",
    "#     custom_standardization = pickle.load(file) \n",
    "\n",
    "# Load the saved model\n",
    "model_bi_gru = keras.models.load_model('./models/model_bidirectional_gru.keras', custom_objects={'custom_standardization': custom_standardization})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    \"O\": 0,\n",
    "    \"B-TO\": 1,\n",
    "    \"B-FROM\": 2,\n",
    "    \"I-TO\": 3,\n",
    "    \"I-FROM\": 4,\n",
    "    \"PAD\": 5\n",
    "}\n",
    "\n",
    "# Define the reverse vocabulary mapping from integers to labels\n",
    "reverse_vocabulary = {index: label for label, index in vocabulary.items()}\n",
    "\n",
    "def make_prediction(sentence):\n",
    "    pred = model_bi_gru.predict([sentence], verbose=0)[0]\n",
    "\n",
    "    # calculate the real length of the sentence to remove the padding\n",
    "    actual_length = len(sentence.split())\n",
    "\n",
    "    predicted_tags = tf.argmax(pred, axis=-1).numpy()[:actual_length]\n",
    "\n",
    "    labels = [reverse_vocabulary[index] for index in predicted_tags]\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extract_predicted_cities(sentence: str, labels: list):\n",
    "    \"\"\"\n",
    "    Extracts predicted cities from a sentence based on the provided labels.\n",
    "\n",
    "    Args:\n",
    "    - sentence (str): The input sentence.\n",
    "    - labels (list): List of labels corresponding to each word in the sentence.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing predicted cities categorized by label.\n",
    "    \"\"\"\n",
    "    predicted_cities = {\n",
    "        \"B-TO\": [],\n",
    "        \"B-FROM\": [],\n",
    "        \"I-TO\": [],\n",
    "        \"I-FROM\": [],\n",
    "    }\n",
    "\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Iterate through each label and its corresponding word\n",
    "    for label, word in zip(labels, words):\n",
    "        if label in predicted_cities:\n",
    "            predicted_cities[label].append(word)\n",
    "\n",
    "    return predicted_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-TO': ['Toulouse'],\n",
       " 'B-FROM': ['Lyon'],\n",
       " 'I-TO': ['Marco'],\n",
       " 'I-FROM': ['Ricola']}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence = \"Je veux aller de Lyon Ricola à Toulouse Marco\"\n",
    "array_labels = make_prediction(Sentence)\n",
    "extract_predicted_cities(Sentence, array_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGV stations finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH UTILITIES\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "def create_graph(df):\n",
    "    G = nx.from_pandas_edgelist(df,\n",
    "                            source=\"start_station\", \n",
    "                            target=\"end_station\",\n",
    "                            edge_attr='duration', # weights\n",
    "                            create_using=nx.DiGraph(oriented=True, data=True))\n",
    "    return G\n",
    "\n",
    "def preload_fastest_paths(G):\n",
    "    \"\"\"\n",
    "    Example: access the shortest path and its weight from node A to node B\n",
    "    path_from_A_to_B = weighted_paths['A']['B']['path']\n",
    "    weight_from_A_to_B = weighted_paths['A']['B']['duration']\n",
    "    \"\"\"\n",
    "    # Johnson's algorithm to find all pairs shortest paths\n",
    "    shortest_paths_johnson = nx.johnson(G, weight=\"duration\")\n",
    "    # Create a dictionary to store the paths with their weights\n",
    "    weighted_paths = {}\n",
    "    for source, targets in shortest_paths_johnson.items():\n",
    "        weighted_paths[source] = {}\n",
    "        for target, path in targets.items():\n",
    "            weight = sum(G[path[i]][path[i + 1]]['duration'] for i in range(len(path) - 1))\n",
    "            weighted_paths[source][target] = {'path': path, 'duration': weight}\n",
    "    return weighted_paths\n",
    "\n",
    "def find_fastest_paths(start_stations, end_stations):\n",
    "    best_path = None\n",
    "    shortest_duration = float('inf')\n",
    "    all_paths_info = []\n",
    "\n",
    "    for start in start_stations:\n",
    "        for end in end_stations:\n",
    "            if start in shortest_paths and end in shortest_paths[start]:\n",
    "                duration = shortest_paths[start][end]['duration']\n",
    "                \n",
    "                path_info = {\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"duration\": duration,\n",
    "                    \"path\": shortest_paths[start][end]['path']\n",
    "                }\n",
    "                all_paths_info.append(path_info)\n",
    "                \n",
    "                if duration < shortest_duration:\n",
    "                    shortest_duration = duration\n",
    "                    best_path = (start, end, duration)\n",
    "    return best_path, all_paths_info\n",
    "\n",
    "# Create graph and preload paths\n",
    "timetables_df = pd.read_csv('./data/timetables.csv', delimiter=',', encoding='utf8')\n",
    "G = create_graph(timetables_df)\n",
    "shortest_paths = preload_fastest_paths(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('lyon part dieu', 'paris montparnasse hall 1 - 2', 327.0),\n",
       " [{'start': 'lyon part dieu',\n",
       "   'end': 'paris montparnasse hall 1 - 2',\n",
       "   'duration': 327.0,\n",
       "   'path': ['lyon part dieu', 'rennes', 'paris montparnasse hall 1 - 2']}])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_path, all_paths = find_fastest_paths(['lyon part dieu'], ['paris montparnasse hall 1 - 2'])\n",
    "best_path, all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris montparnasse hall 1 - 2' 'bordeaux saint-jean' 'strasbourg'\n",
      " 'paris est' 'paris gare du nord' 'aix-en-provence tgv'\n",
      " 'marseille saint-charles' 'montpellier saint-roch' 'metz'\n",
      " 'nancy place de la république' 'nice-ville' 'toulon' 'reims' 'colmar'\n",
      " 'remiremont' 'saint-dié-des-vosges' 'sedan' 'charleville-mézières'\n",
      " 'bar-le-duc' 'luxembourg' 'thionville' 'paris gare de lyon hall 1 - 2'\n",
      " 'lille europe' 'lyon part dieu' 'montpellier sud de france' 'rennes'\n",
      " 'lyon perrache' 'lille flandres' 'nantes' 'le havre' 'perpignan' 'hyères'\n",
      " 'miramas' 'avignon centre' 'grenoble' 'évian-les-bains'\n",
      " 'saint-étienne châteaucreux' 'mulhouse' 'besançon viotte'\n",
      " 'toulouse matabiau' 'annecy' 'valenciennes' 'tourcoing' 'dunkerque'\n",
      " 'boulogne sur mer' 'rang-du-fliers - verton - berck' 'brest' 'quimper'\n",
      " 'auray' 'la rochelle' 'niort' 'aéroport charles de gaulle 2 tgv'\n",
      " 'saint-malo' 'tours' 'poitiers' 'arcachon' 'agen' 'hendaye' 'tarbes'\n",
      " 'saint-brieuc' 'lannion' 'vannes' 'lorient' 'saint-nazaire' 'le croisic'\n",
      " \"les sables-d'olonne\" 'la roche-sur-yon' 'zuerich hb'\n",
      " 'chambéry - challes-les-eaux' 'lausanne' 'freiburg (breisgau) hbf'\n",
      " 'stuttgart hbf' 'munich' 'francfort sur le main' 'karlsruhe hbf'\n",
      " 'barcelone-sants' 'geneve' 'bruxelles midi' 'dijon' 'calais ville'\n",
      " 'le mans' 'basel sbb']\n"
     ]
    }
   ],
   "source": [
    "# Get unique stations available\n",
    "start_stations_unique = timetables_df['start_station'].unique()\n",
    "end_stations_unique = timetables_df['end_station'].unique()\n",
    "all_stations = list(start_stations_unique) + list(end_stations_unique)\n",
    "unique_stations = pd.unique(all_stations)\n",
    "print(unique_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def find_similar_stations(city, stations=unique_stations, threshold=90):\n",
    "    \"\"\"\n",
    "    Find stations similar to the given city name.\n",
    "\n",
    "    Parameters:\n",
    "        city (str): The city name to search for.\n",
    "        stations (list): List of station names.\n",
    "        threshold (int): Similarity threshold for fuzzy string matching.\n",
    "\n",
    "    Returns:\n",
    "        list: List of stations similar to the city name.\n",
    "    \"\"\"\n",
    "    # Convert city name to lowercase\n",
    "    city_lower = city.lower()\n",
    "\n",
    "    # Find stations similar to the city\n",
    "    similar_stations = [station for station in stations if fuzz.partial_ratio(city_lower, station.lower()) >= threshold]\n",
    "\n",
    "    return similar_stations[:4] # return maximum of 4 stations (paris has 4 stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris montparnasse hall 1 - 2',\n",
       " 'paris est',\n",
       " 'paris gare du nord',\n",
       " 'paris gare de lyon hall 1 - 2']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_stations('paris', threshold=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_city_combinations(b_cities, i_cities):\n",
    "    \"\"\"\n",
    "    Generate all possible combinations of cities\n",
    "    \"\"\"\n",
    "    \n",
    "    city_combinations = [[b_city] + i_cities for b_city in b_cities]\n",
    "    return city_combinations\n",
    "\n",
    "def find_station_for_direction(predicted_cities: dict[str, list], direction='TO', threshold=90):\n",
    "    \"\"\"\n",
    "    Find a similar station for a given direction based on predicted cities.\n",
    "\n",
    "    Args:\n",
    "    - predicted_cities (dict): Dictionary containing predicted cities categorized by label.\n",
    "    - direction (str): Direction ('TO' or 'FROM').\n",
    "    - threshold (int): Similarity threshold for finding similar stations.\n",
    "\n",
    "    Returns:\n",
    "    - str: The found similar station, or None if no similar station is found.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate B-TO and I-TO cities if they exist\n",
    "    if direction == 'TO':\n",
    "        city_combinations =  generate_city_combinations(predicted_cities.get('B-TO', []), predicted_cities.get('I-TO', []))\n",
    "    else:\n",
    "        city_combinations =  generate_city_combinations(predicted_cities.get('B-FROM', []), predicted_cities.get('I-FROM', []))\n",
    "    \n",
    "    print(city_combinations)\n",
    "    for city_words in city_combinations:\n",
    "        # Try different lengths of city names starting from the full city name\n",
    "        for i in range(len(city_words), 0, -1):\n",
    "            # Concatenate the first i words of the city name\n",
    "            partial_city = ' '.join(city_words[:i])\n",
    "            similar_station = find_similar_stations(partial_city, threshold=threshold)\n",
    "            if similar_station:\n",
    "                return similar_station\n",
    "             \n",
    "    # If no similar station is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a']]\n",
      "[['ok', 'a', 'b'], ['lyon', 'a', 'b']]\n",
      "['paris montparnasse hall 1 - 2', 'bordeaux saint-jean', 'strasbourg', 'paris est', 'paris gare du nord']\n",
      "['paris gare de lyon hall 1 - 2', 'lyon part dieu', 'lyon perrache']\n"
     ]
    }
   ],
   "source": [
    "predicted_cities = {'B-TO': ['ok', 'lyon'], 'B-FROM': ['a'], 'I-TO': [\"a\", \"b\"], 'I-FROM': []}\n",
    "\n",
    "stations_from = find_station_for_direction(predicted_cities, direction='FROM', threshold=90)\n",
    "stations_to = find_station_for_direction(predicted_cities, direction='TO', threshold=90)\n",
    "\n",
    "print(stations_from)\n",
    "print(stations_to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "\n",
    "- language-tool-python (spell checker) (https://pypi.org/project/language-tool-python/)\n",
    "- Vosk (https://alphacephei.com/vosk/) (https://alphacephei.com/vosk/models/vosk-model-fr-0.22.zip)\n",
    "- PyAudio (https://people.csail.mit.edu/hubert/pyaudio/)\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Tensorflow\n",
    "- fuzzywuzzy (https://github.com/seatgeek/fuzzywuzzy)\n",
    "- python-Levenshtein (https://github.com/rapidfuzz/python-Levenshtein)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment aller de Lyon à Marseille\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m corrected_sentence \u001b[38;5;241m=\u001b[39m spell_check(Sentence)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(corrected_sentence)\n\u001b[1;32m----> 6\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prediction\u001b[49m(corrected_sentence)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[0;32m      9\u001b[0m predicted_cities \u001b[38;5;241m=\u001b[39m extract_predicted_cities(corrected_sentence, labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "Sentence = \"cmment aller de lyon a marseille\"\n",
    "\n",
    "corrected_sentence = spell_check(Sentence)\n",
    "print(corrected_sentence)\n",
    "\n",
    "labels = make_prediction(corrected_sentence)\n",
    "print(labels)\n",
    "\n",
    "predicted_cities = extract_predicted_cities(corrected_sentence, labels)\n",
    "print(predicted_cities)\n",
    "\n",
    "stations_from = find_station_for_direction(predicted_cities, direction='FROM', threshold=90)\n",
    "stations_to = find_station_for_direction(predicted_cities, direction='TO', threshold=90)\n",
    "print(stations_from, stations_to)\n",
    "\n",
    "for station_from in stations_from:\n",
    "    for station_to in stations_to:\n",
    "        best_path, all_paths = find_fastest_paths(station_from, station_to)\n",
    "        print(best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "depuis toulouse comment aller à lyon\n",
      "Depuis Toulouse comment aller à Lyon\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'B-FROM', 'O', 'O', 'O', 'B-TO']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence = start_voice_recognition()\n",
    "\n",
    "corrected_sentence = spell_check(Sentence)\n",
    "print(corrected_sentence)\n",
    "\n",
    "labels = make_prediction(corrected_sentence)\n",
    "print(labels)\n",
    "\n",
    "predicted_cities = extract_predicted_cities(corrected_sentence, labels)\n",
    "print(predicted_cities)\n",
    "\n",
    "stations_from = find_station_for_direction(predicted_cities, direction='FROM', threshold=90)\n",
    "stations_to = find_station_for_direction(predicted_cities, direction='TO', threshold=90)\n",
    "print(stations_from, stations_to)\n",
    "\n",
    "for station_from in stations_from:\n",
    "    for station_to in stations_to:\n",
    "        best_path, all_paths = find_fastest_paths(station_from, station_to)\n",
    "        print(best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
