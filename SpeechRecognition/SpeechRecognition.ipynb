{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are only 5 open-source licenses that should ever be used:\n",
    "\n",
    "- **Apache 2.0**, for when you want to allow people to make and release proprietary versions of your product. It is advisable to only use this for extremely-low-level libraries/runtimes.\n",
    "\n",
    "- **MPL 2.0**, but only if it does NOT invoke the \"incompatible with secondary licenses\" clause. This allows use as part of proprietary products, but preserves the sanctity of source code files only.\n",
    "\n",
    "- **LGPL 3+**, for when you want your library to be usable as an upgradable part of proprietary products, but your code itself must remain pure.\n",
    "\n",
    "    - it is possible to add a \"static linking exception\", creating a situation similar to the MPL (think about it: what is the difference between a library and (a collection of) single files?) but with a better-known basis. However, you must be aware that this requires giving up the \"able to upgrade\" freedom, and is only encouraging bad practices. Seriously, RPATH isn't that hard; the corner cases are well-documented.\n",
    "\n",
    "- **GPL 3+**, for applications that run on an individual computer.\n",
    "\n",
    "- **AGPL 3+**, for applications that run on the network AND are worth the effort of complying with the distribution requirements. This ensures continued freedom under the maximum set of conditions, but can be painful. If you choose this, you must figure out how to make every build of your software point to a publicly-accessible version of the source (specifying a commit is good, but remember: you can't assume a single central git repo, since there may be temporary forks; additionally think of what Linux distributions want to do. Your ./configure should mandate passing several options like --vendor-url at the very least), which implies significant (but arguably sensible) workflow restrictions.\n",
    "\n",
    "\n",
    "For our project, our objective is to produce an application for public purposes. **Apache 2.0** licence is preferable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library used:\n",
    "- **pydub**: Resample audio\n",
    "- **sounddevice**: record audio\n",
    "- **Vosk**: Speech Recognition open-source (apache 2.0)\n",
    "- **ipytest**: Python Test in jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording audio\n",
    "\n",
    "https://python-sounddevice.readthedocs.io/en/0.4.6/api/index.html\n",
    "\n",
    "sounddevice simplifies the recording process, easier of pyaudio.\n",
    "MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished. Saving to file...\n"
     ]
    }
   ],
   "source": [
    "# Record audio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "samplerate = 16000  # Hertz\n",
    "duration = 5  # seconds\n",
    "filename = 'output.wav'\n",
    "\n",
    "print(\"Recording...\")\n",
    "mydata = sd.rec(int(samplerate * duration), \n",
    "                samplerate=samplerate,\n",
    "                channels=1, # mono (Vosk) / stereo\n",
    "                dtype='int16')\n",
    "sd.wait()\n",
    "print(\"Recording finished. Saving to file...\")\n",
    "\n",
    "# Save as WAV file using scipy\n",
    "write(filename, samplerate, mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample audio\n",
    "For optimal results, we always use audio at 16kHz sample rate, mono, and wave type.\n",
    "\n",
    "⚠️You need **ffmep** installed on your system:\n",
    "https://github.com/BtbN/FFmpeg-Builds/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def resample_audio(audio_path:str, output_path: str =\"resampled_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Resample an audio file to wav, 16khz, and mono\n",
    "\n",
    "    Returns: \n",
    "        - output_path\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    #audio = AudioSegment.from_wav(input_path)\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    \n",
    "    # Resample it\n",
    "    resampled_audio = audio.set_frame_rate(16000) # Vosk and Other models better works with 16kHz\n",
    "\n",
    "    # Convert the audio file to single channel (mono) (Vosk)\n",
    "    resampled_audio = resampled_audio.set_channels(1)\n",
    "    \n",
    "    # Export the resampled audio\n",
    "    resampled_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vosk\n",
    "\n",
    "- Open-source\n",
    "- offline\n",
    "- supports 10+ language including french\n",
    "\n",
    "https://alphacephei.com/vosk\n",
    "https://alphacephei.com/vosk/models\n",
    "\n",
    "- we chose the french model: **vosk-model-fr-0.22 (Apache 2.0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model\n",
    "\n",
    "# Load the model\n",
    "model = Model(\"./models/vosk-model-fr-0.22\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live speech transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from ./models/vosk-model-fr-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from ./models/vosk-model-fr-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from ./models/vosk-model-fr-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo ./models/vosk-model-fr-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from ./models/vosk-model-fr-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from ./models/vosk-model-fr-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from ./models/vosk-model-fr-0.22/rnnlm/final.raw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||PaMacCore (AUHAL)|| Error on line 2523: err='-50', msg=Unknown Error\n",
      "test\n",
      "voiture\n",
      "coca-cola\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "import pyaudio\n",
    "\n",
    "# model = Model(\"./vosk-model-fr-0.22\")\n",
    "model = Model(\"./models/vosk-model-fr-0.22\")\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "mic = pyaudio.PyAudio()\n",
    "stream = mic.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=8192)\n",
    "stream.start_stream()\n",
    "\n",
    "while True:\n",
    "    data = stream.read(4096)\n",
    "    # if len(data) == 0:\n",
    "    #     break\n",
    "    if recognizer.AcceptWaveform(data):\n",
    "        text = recognizer.Result()\n",
    "        # print(text)\n",
    "        print(text[14:-3])\n",
    "        if text[14:-3] == \"fin\":\n",
    "            break\n",
    "    # else:\n",
    "    #     print(recognizer.PartialResult())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcription from audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\" : \"pourriez-vous m'indiquer le chemin pour aller de paris à marseille s'il vous plaît\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from vosk import KaldiRecognizer\n",
    "import wave\n",
    "\n",
    "# Prepare audio\n",
    "audio_file = \"./audio_files/test_french.m4a\"\n",
    "audio_file = resample_audio(audio_file, \"./test_french.m4a\")\n",
    "\n",
    "# Create a recognizer object\n",
    "rec = KaldiRecognizer(model, 16000)  # Replace with the sample rate of your audio\n",
    "\n",
    "\n",
    "# Process an audio file\n",
    "with wave.open(audio_file, \"rb\") as f:\n",
    "    while True:\n",
    "        data = f.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        rec.AcceptWaveform(data)\n",
    "\n",
    "# Print the final transcription\n",
    "print(rec.FinalResult())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Now we have a good model, we need to test it with lots of samples.\n",
    "\n",
    "We created some audios. OUr objective is to identify correctly the city A and the city B\n",
    "\n",
    "1) First we need to get how test audio data.\n",
    "\n",
    "Download these these files to zip:\n",
    "- https://drive.google.com/drive/folders/1ir7eqefODLuBq4Rw1rV7cJG1era-qzB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to ./audio_files/test\n",
      "Test folder data: ./audio_files/test\\voice_recognition_data\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# zip file name\n",
    "zip_file = \"voice_recognition_data-20230928T150022Z-001.zip\"\n",
    "\n",
    "# directory name where files will be extracted\n",
    "TEST_FOLDER = \"./audio_files/test\"\n",
    "\n",
    "# Create the directory where the files will be extracted\n",
    "os.makedirs(TEST_FOLDER, exist_ok=True)\n",
    "\n",
    "# Create a ZipFile object\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file into the directory\n",
    "    zip_ref.extractall(TEST_FOLDER)\n",
    "\n",
    "print(f\"Files extracted to {TEST_FOLDER}\")\n",
    "\n",
    "# update test DIR\n",
    "TEST_FOLDER = os.path.join(TEST_FOLDER, \"voice_recognition_data\")\n",
    "\n",
    "print(f\"Test folder data: {TEST_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all audio files into one folder for test\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "src_directory = \"./audio_files/test/voice_recognition_data/audio_files/\"\n",
    "target_directory = \"./audio_files/test/voice_recognition_data/all\"\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)\n",
    "\n",
    "# Walking through all files in all subdirectories of the source directory\n",
    "for foldername, subfolders, filenames in os.walk(src_directory):\n",
    "    for filename in filenames:\n",
    "        # Assuming audio files have extensions .mp3, .wav, .m4a. Add more extensions as needed\n",
    "        if filename.endswith(('.mp3', '.wav', '.m4a')):\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            shutil.copy(file_path, target_directory)  # Move each audio file to the target directory\n",
    "\n",
    "# remove whitespaces\n",
    "for filename in os.listdir(target_directory):\n",
    "    # check if the filename contains a space\n",
    "    if ' ' in filename:\n",
    "        # create a new filename by replacing spaces with no space\n",
    "        new_filename = filename.replace(' ', '')\n",
    "        # create the full paths to the old and new filenames\n",
    "        old_path = os.path.join(target_directory, filename)\n",
    "        new_path = os.path.join(target_directory, new_filename)\n",
    "        # rename the file\n",
    "        os.rename(old_path, new_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now we can use ipytest to do Unit test in jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import json\n",
    "from vosk import KaldiRecognizer\n",
    "\n",
    "def transcript(audio_file) -> dict:\n",
    "    # Prepare audio\n",
    "    audio_file = resample_audio(audio_file, \"./resampled_audio.wav\")\n",
    "\n",
    "    # Create a recognizer object\n",
    "    rec = KaldiRecognizer(model, 16000)  # Replace with the sample rate of your audio\n",
    "\n",
    "    # Process an audio file\n",
    "    with wave.open(audio_file, \"rb\") as f:\n",
    "        while True:\n",
    "            data = f.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            rec.AcceptWaveform(data)\n",
    "\n",
    "    # Print the final transcription\n",
    "    return json.loads(rec.FinalResult())[\"text\"] # returns a JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pourriez-vous m'indiquer le chemin pour aller de paris à marseille s'il vous plaît\n",
      "['paris', 'marseille']\n",
      "je suis à lyon et je dois me rendre à toulouse quel est le meilleur itinéraire\n",
      "['lyon', 'toulouse']\n",
      "excusez-moi pouvez-vous me dire comment aller de bordeaux à nice\n",
      "['bordeaux', 'nice']\n",
      "je cherche à aller de nantes à strasbourg vous m'aider avec cette direction\n",
      "['nantes', 'strasbourg']\n",
      "quelle est la meilleure façon de se rendre de lille à montpellier en voiture\n",
      "['lille', 'montpellier']\n",
      "savez-vous comment je peux aller de grenoble à rennes\n",
      "['grenoble', 'rennes']\n",
      "je planifie un voyage de tours à nancy quelle route marocain m'entendez-vous\n",
      "['tours', 'nancy']\n",
      "je me demande quel est le trajet le plus rapide pour aller d'orléans à dijon joue des suggestions\n",
      "['orléans', 'dijon']\n",
      "pouvez-vous ajouter les directions pour aller de rouen à avignon\n",
      "['rouen', 'avignon']\n",
      "je souhaite voyager de brest à amiens quel chemin devrais-je prendre\n",
      "['brest', 'amiens']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "DIRECTORY = os.path.join(TEST_FOLDER, \"audio_files\", \"IA_voice_1_a_10\")\n",
    "\n",
    "# read the excel\n",
    "audio_files_df = pd.read_excel(os.path.join(TEST_FOLDER, \"audio_files.xlsx\"))\n",
    "\n",
    "file_names = audio_files_df[\"file_name\"].values\n",
    "scripts = audio_files_df[\"script\"].values\n",
    "tags = audio_files_df[\"tags_to_recover\"].values\n",
    "\n",
    "# Converting each string into a list of cities\n",
    "city_lists = [tag.split(', ') for tag in tags]\n",
    "\n",
    "\n",
    "# Flattening the list of lists to get a single list of all cities\n",
    "all_cities = [city for sublist in city_lists for city in sublist]\n",
    "\n",
    "for file_name, tag in zip(file_names, city_lists):\n",
    "    # find audio file\n",
    "    audio_file = os.path.join(DIRECTORY, str(file_name))\n",
    "    if not os.path.exists(audio_file): \n",
    "        #print(f\"Non existant file: {audio_file}.\"); \n",
    "        continue;\n",
    "\n",
    "    # Transcript audio file\n",
    "    predicted_sentence = transcript(audio_file)\n",
    "\n",
    "    # Create a pattern that matches any city name in the list of all cities\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(all_cities) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    # Find all city names in the sentence\n",
    "    found_cities = re.findall(pattern, predicted_sentence)\n",
    "    \n",
    "    print(predicted_sentence)\n",
    "    print(found_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.10.10, pytest-7.4.2, pluggy-1.3.0 -- c:\\Users\\loann\\.conda\\envs\\global\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\loann\\Desktop\\Epitech\\AI\\T-AIA-901-LYO_1\\SpeechRecognition\n",
      "plugins: anyio-3.6.2, requests-mock-1.11.0, typeguard-2.13.3\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "t_6a7971e557384e9ea08aa310d8e126f7.py::test_speech_recognition \u001b[91mExpected ['lyon', 'lille'] !== ['lyon'] for audio: audio_files/test/voice_recognition_data/all/49.wav.\n",
      "  Predicted sentence: donc j'aimerais savoir comment atteindre lyon en venant de l'île pourriez-vous m'orienter\n",
      "assert ['lyon'] == ['lyon', 'lille']\n",
      "  Right contains one more item: 'lille'\n",
      "  Full diff:\n",
      "  - ['lyon', 'lille']\n",
      "  + ['lyon']\u001b[0m\n",
      "\u001b[91mExpected ['toulouse', 'bordeaux'] !== ['bordeaux'] for audio: audio_files/test/voice_recognition_data/all/53.m4a.\n",
      "  Predicted sentence: ergonomique depuis bordeaux\n",
      "assert ['bordeaux'] == ['toulouse', 'bordeaux']\n",
      "  At index 0 diff: 'bordeaux' != 'toulouse'\n",
      "  Right contains one more item: 'bordeaux'\n",
      "  Full diff:\n",
      "  - ['toulouse', 'bordeaux']\n",
      "  + ['bordeaux']\u001b[0m\n",
      "\u001b[91mExpected ['nancy', 'grenoble'] !== ['nancy'] for audio: audio_files/test/voice_recognition_data/all/55.m4a.\n",
      "  Predicted sentence: nancy quel chemin\n",
      "assert ['nancy'] == ['nancy', 'grenoble']\n",
      "  Right contains one more item: 'grenoble'\n",
      "  Full diff:\n",
      "  - ['nancy', 'grenoble']\n",
      "  + ['nancy']\u001b[0m\n",
      "\u001b[91mExpected ['colmar', 'brive la gaillarde'] !== [] for audio: audio_files/test/voice_recognition_data/all/59.m4a.\n",
      "  Predicted sentence: tu es calmé à brive-la-gaillarde\n",
      "assert [] == ['colmar', 'b...la gaillarde']\n",
      "  Right contains 2 more items, first extra item: 'colmar'\n",
      "  Full diff:\n",
      "  - ['colmar', 'brive la gaillarde']\n",
      "  + []\u001b[0m\n",
      "\u001b[91mExpected ['sarlat', 'périgueux'] !== ['périgueux'] for audio: audio_files/test/voice_recognition_data/all/73.m4a.\n",
      "  Predicted sentence: peux-tu me dire comment aller de salat asptt périgueux\n",
      "assert ['périgueux'] == ['sarlat', 'périgueux']\n",
      "  At index 0 diff: 'périgueux' != 'sarlat'\n",
      "  Right contains one more item: 'périgueux'\n",
      "  Full diff:\n",
      "  - ['sarlat', 'périgueux']\n",
      "  + ['périgueux']\u001b[0m\n",
      "\u001b[91mExpected ['bar-le-duc', 'épinal'] !== ['bar-le-duc', 'epinal'] for audio: audio_files/test/voice_recognition_data/all/74.m4a.\n",
      "  Predicted sentence: quel est le meilleur trajet pour aller de bar-le-duc à epinal\n",
      "assert ['bar-le-duc', 'epinal'] == ['bar-le-duc', 'épinal']\n",
      "  At index 1 diff: 'epinal' != 'épinal'\n",
      "  Full diff:\n",
      "  - ['bar-le-duc', 'épinal']\n",
      "  ?                 ^\n",
      "  + ['bar-le-duc', 'epinal']\n",
      "  ?                 ^\u001b[0m\n",
      "\u001b[91mExpected ['alençon', 'carentan'] !== ['alençon'] for audio: audio_files/test/voice_recognition_data/all/76.m4a.\n",
      "  Predicted sentence: comment puis-je me rendre d'alençon à quarante ans\n",
      "assert ['alençon'] == ['alençon', 'carentan']\n",
      "  Right contains one more item: 'carentan'\n",
      "  Full diff:\n",
      "  - ['alençon', 'carentan']\n",
      "  + ['alençon']\u001b[0m\n",
      "\u001b[91mExpected ['millau', 'rodez'] !== ['rodez'] for audio: audio_files/test/voice_recognition_data/all/77.m4a.\n",
      "  Predicted sentence: quel est le chemin pour aller de milo à rodez\n",
      "assert ['rodez'] == ['millau', 'rodez']\n",
      "  At index 0 diff: 'rodez' != 'millau'\n",
      "  Right contains one more item: 'rodez'\n",
      "  Full diff:\n",
      "  - ['millau', 'rodez']\n",
      "  + ['rodez']\u001b[0m\n",
      "\u001b[91mExpected ['foix', 'carcassonne'] !== ['carcassonne'] for audio: audio_files/test/voice_recognition_data/all/78.m4a.\n",
      "  Predicted sentence: donne-moi l'itinéraire pour aller deux fois à carcassonne\n",
      "assert ['carcassonne'] == ['foix', 'carcassonne']\n",
      "  At index 0 diff: 'carcassonne' != 'foix'\n",
      "  Right contains one more item: 'carcassonne'\n",
      "  Full diff:\n",
      "  - ['foix', 'carcassonne']\n",
      "  ?  --------\n",
      "  + ['carcassonne']\u001b[0m\n",
      "City Accuracy: 92.65% (126/136)\n",
      "Audio Accuracy: 86.76% (59/68)\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "t_6a7971e557384e9ea08aa310d8e126f7.py::test_speech_recognition_without_noise_suppression \u001b[91mExpected ['lyon', 'lille'] !== ['lyon'] for audio: audio_files/test/voice_recognition_data/all/49.wav.\n",
      "  Predicted sentence: donc j'aimerais savoir comment atteindre lyon en venant de l'île pourriez-vous m'orienter\n",
      "assert ['lyon'] == ['lyon', 'lille']\n",
      "  Right contains one more item: 'lille'\n",
      "  Full diff:\n",
      "  - ['lyon', 'lille']\n",
      "  + ['lyon']\u001b[0m\n",
      "\u001b[91mExpected ['toulouse', 'bordeaux'] !== ['bordeaux'] for audio: audio_files/test/voice_recognition_data/all/53.m4a.\n",
      "  Predicted sentence: ergonomique depuis bordeaux\n",
      "assert ['bordeaux'] == ['toulouse', 'bordeaux']\n",
      "  At index 0 diff: 'bordeaux' != 'toulouse'\n",
      "  Right contains one more item: 'bordeaux'\n",
      "  Full diff:\n",
      "  - ['toulouse', 'bordeaux']\n",
      "  + ['bordeaux']\u001b[0m\n",
      "\u001b[91mExpected ['nancy', 'grenoble'] !== ['nancy'] for audio: audio_files/test/voice_recognition_data/all/55.m4a.\n",
      "  Predicted sentence: nancy quel chemin\n",
      "assert ['nancy'] == ['nancy', 'grenoble']\n",
      "  Right contains one more item: 'grenoble'\n",
      "  Full diff:\n",
      "  - ['nancy', 'grenoble']\n",
      "  + ['nancy']\u001b[0m\n",
      "\u001b[91mExpected ['colmar', 'brive la gaillarde'] !== [] for audio: audio_files/test/voice_recognition_data/all/59.m4a.\n",
      "  Predicted sentence: tu es calmé à brive-la-gaillarde\n",
      "assert [] == ['colmar', 'b...la gaillarde']\n",
      "  Right contains 2 more items, first extra item: 'colmar'\n",
      "  Full diff:\n",
      "  - ['colmar', 'brive la gaillarde']\n",
      "  + []\u001b[0m\n",
      "\u001b[91mExpected ['sarlat', 'périgueux'] !== ['périgueux'] for audio: audio_files/test/voice_recognition_data/all/73.m4a.\n",
      "  Predicted sentence: peux-tu me dire comment aller de salat asptt périgueux\n",
      "assert ['périgueux'] == ['sarlat', 'périgueux']\n",
      "  At index 0 diff: 'périgueux' != 'sarlat'\n",
      "  Right contains one more item: 'périgueux'\n",
      "  Full diff:\n",
      "  - ['sarlat', 'périgueux']\n",
      "  + ['périgueux']\u001b[0m\n",
      "\u001b[91mExpected ['bar-le-duc', 'épinal'] !== ['bar-le-duc', 'epinal'] for audio: audio_files/test/voice_recognition_data/all/74.m4a.\n",
      "  Predicted sentence: quel est le meilleur trajet pour aller de bar-le-duc à epinal\n",
      "assert ['bar-le-duc', 'epinal'] == ['bar-le-duc', 'épinal']\n",
      "  At index 1 diff: 'epinal' != 'épinal'\n",
      "  Full diff:\n",
      "  - ['bar-le-duc', 'épinal']\n",
      "  ?                 ^\n",
      "  + ['bar-le-duc', 'epinal']\n",
      "  ?                 ^\u001b[0m\n",
      "\u001b[91mExpected ['alençon', 'carentan'] !== ['alençon'] for audio: audio_files/test/voice_recognition_data/all/76.m4a.\n",
      "  Predicted sentence: comment puis-je me rendre d'alençon à quarante ans\n",
      "assert ['alençon'] == ['alençon', 'carentan']\n",
      "  Right contains one more item: 'carentan'\n",
      "  Full diff:\n",
      "  - ['alençon', 'carentan']\n",
      "  + ['alençon']\u001b[0m\n",
      "\u001b[91mExpected ['millau', 'rodez'] !== ['rodez'] for audio: audio_files/test/voice_recognition_data/all/77.m4a.\n",
      "  Predicted sentence: quel est le chemin pour aller de milo à rodez\n",
      "assert ['rodez'] == ['millau', 'rodez']\n",
      "  At index 0 diff: 'rodez' != 'millau'\n",
      "  Right contains one more item: 'rodez'\n",
      "  Full diff:\n",
      "  - ['millau', 'rodez']\n",
      "  + ['rodez']\u001b[0m\n",
      "\u001b[91mExpected ['foix', 'carcassonne'] !== ['carcassonne'] for audio: audio_files/test/voice_recognition_data/all/78.m4a.\n",
      "  Predicted sentence: donne-moi l'itinéraire pour aller deux fois à carcassonne\n",
      "assert ['carcassonne'] == ['foix', 'carcassonne']\n",
      "  At index 0 diff: 'carcassonne' != 'foix'\n",
      "  Right contains one more item: 'carcassonne'\n",
      "  Full diff:\n",
      "  - ['foix', 'carcassonne']\n",
      "  ?  --------\n",
      "  + ['carcassonne']\u001b[0m\n",
      "City Accuracy: 92.65% (126/136)\n",
      "Audio Accuracy: 86.76% (59/68)\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 80.07s (0:01:20)\u001b[0m\u001b[32m ===================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipytest\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Configure logging\n",
    "#logging.basicConfig(filename='test.log', level=logging.DEBUG, filemode='w')\n",
    "\n",
    "\n",
    "DIRECTORY = \"audio_files/test/voice_recognition_data/all/\"\n",
    "\n",
    "audio_files = pd.read_excel(\"./audio_files/test/voice_recognition_data/audio_files.xlsx\")\n",
    "file_names = audio_files[\"file_name\"].values\n",
    "scripts = audio_files[\"script\"].values\n",
    "tags = audio_files[\"tags_to_recover\"].values\n",
    "\n",
    "# Converting each string into a list of cities\n",
    "city_lists = [tag.lower().split(', ') for tag in tags]\n",
    "\n",
    "# Lowercase all strings\n",
    "\n",
    "# Flattening the list of lists to get a single list of all cities\n",
    "all_cities = [city for sublist in city_lists for city in sublist]\n",
    "\n",
    "# Create a pattern that matches any city name in the list of all cities\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(all_cities) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "#log_msg = f\"File: {audio_file}, Expected: {tag}, Found: {found_cities}. Predicted sentence: {sentence}\"\n",
    "#logging.info(log_msg)\n",
    "\n",
    "def compute_accuracy(correct, total):\n",
    "    if total == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return correct / total\n",
    "\n",
    "def calculate_score(correct_cities, total_cities, correct_audios, total_audios):\n",
    "    \"\"\"# Compute and print the accuracies\"\"\"\n",
    "    city_accuracy = compute_accuracy(correct_cities, total_cities) * 100\n",
    "    audio_accuracy = compute_accuracy(correct_audios, total_audios) * 100\n",
    "    \n",
    "    print(f\"City Accuracy: {city_accuracy:.2f}% ({correct_cities}/{total_cities})\")\n",
    "    print(f\"Audio Accuracy: {audio_accuracy:.2f}% ({correct_audios}/{total_audios})\")\n",
    "\n",
    "# Define the test case\n",
    "def test_speech_recognition():\n",
    "    total_cities = 0\n",
    "    correct_cities = 0\n",
    "    total_audios = 0\n",
    "    correct_audios = 0\n",
    "\n",
    "    for file_name, tag in zip(file_names, city_lists):\n",
    "        audio_file = DIRECTORY + str(file_name)\n",
    "\n",
    "        # Check if the audio_file is existant\n",
    "        if not os.path.exists(audio_file): \n",
    "            continue;\n",
    "        \n",
    "        # transcript\n",
    "        sentence = transcript(audio_file)\n",
    "\n",
    "        # Find all city names in the sentence\n",
    "        found_cities = re.findall(pattern, sentence)\n",
    "\n",
    "        # update values for scoring\n",
    "        total_audios +=1\n",
    "        total_cities += len(tag)\n",
    "        correct_cities += sum(1 for city in found_cities if city.lower() in tag)\n",
    "        if set(found_cities) == set(tag):\n",
    "            correct_audios += 1\n",
    "\n",
    "        try:\n",
    "            assert found_cities == tag, f\"Expected {tag} !== {found_cities} for audio: {audio_file}.\\nPredicted sentence: {sentence}\"\n",
    "        except AssertionError as e:\n",
    "            print('\\033[91m' + str(e) + '\\033[0m')\n",
    "\n",
    "    calculate_score(correct_cities, total_cities, correct_audios, total_audios)\n",
    "\n",
    "# Run the tests\n",
    "ipytest.run(\"--full-trace\", \"-vv\", \"-s\")\n",
    "\n",
    "# -s allow print statement\n",
    "# -vv long verbose\n",
    "# -q quiet / -qq more quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du test\n",
    "Notre modèle identifie a 91% les bonnes villes et 84% toutes les villes bonnes dans les audios.\n",
    "\n",
    "- Notre modèle identifie mal la vile **Lille** avec le mot **l'île**. Pour l'audio (49)\n",
    "- Pour l'audio 49, 53, 59, il identifie mal / oublie les villes / mots. Les bruits ambients affectent ces résultats.\n",
    "\n",
    "- Audio 73: \"Sarlat\", p= \"salat\". Il faut Bien prononcer le \"r\"\n",
    "- Audio 76, \"Carentan\", p= \"quarante ans\"\n",
    "- Audio 77, \"Millau\", p= \"milo\". \"Prononcé \"milo\" dans le script.\n",
    "- Audio 78, \"Foix\", p= \"deux fois\". Dire \"à Foix\" fonctionne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpeechRecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
