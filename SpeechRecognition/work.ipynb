{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are only 5 open-source licenses that should ever be used:\n",
    "\n",
    "- **Apache 2.0**, for when you want to allow people to make and release proprietary versions of your product. It is advisable to only use this for extremely-low-level libraries/runtimes.\n",
    "\n",
    "- **MPL 2.0**, but only if it does NOT invoke the \"incompatible with secondary licenses\" clause. This allows use as part of proprietary products, but preserves the sanctity of source code files only.\n",
    "\n",
    "- **LGPL 3+**, for when you want your library to be usable as an upgradable part of proprietary products, but your code itself must remain pure.\n",
    "\n",
    "    - it is possible to add a \"static linking exception\", creating a situation similar to the MPL (think about it: what is the difference between a library and (a collection of) single files?) but with a better-known basis. However, you must be aware that this requires giving up the \"able to upgrade\" freedom, and is only encouraging bad practices. Seriously, RPATH isn't that hard; the corner cases are well-documented.\n",
    "\n",
    "- **GPL 3+**, for applications that run on an individual computer.\n",
    "\n",
    "- **AGPL 3+**, for applications that run on the network AND are worth the effort of complying with the distribution requirements. This ensures continued freedom under the maximum set of conditions, but can be painful. If you choose this, you must figure out how to make every build of your software point to a publicly-accessible version of the source (specifying a commit is good, but remember: you can't assume a single central git repo, since there may be temporary forks; additionally think of what Linux distributions want to do. Your ./configure should mandate passing several options like --vendor-url at the very least), which implies significant (but arguably sensible) workflow restrictions.\n",
    "\n",
    "\n",
    "For our project, our objective is to produce an application for public purposes. **Apache 2.0** licence is preferable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library used:\n",
    "- **pydub**: Resample audio\n",
    "- **sounddevice**: record audio\n",
    "- **Vosk**: Speech Recognition open-source (apache 2.0)\n",
    "- **ipytest**: Python Test in jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording audio\n",
    "\n",
    "https://python-sounddevice.readthedocs.io/en/0.4.6/api/index.html\n",
    "\n",
    "sounddevice simplifies the recording process, easier of pyaudio.\n",
    "MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished. Saving to file...\n"
     ]
    }
   ],
   "source": [
    "# Record audio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "samplerate = 16000  # Hertz\n",
    "duration = 5  # seconds\n",
    "filename = 'output.wav'\n",
    "\n",
    "print(\"Recording...\")\n",
    "mydata = sd.rec(int(samplerate * duration), \n",
    "                samplerate=samplerate,\n",
    "                channels=1, # mono (Vosk) / stereo\n",
    "                dtype='int16')\n",
    "sd.wait()\n",
    "print(\"Recording finished. Saving to file...\")\n",
    "\n",
    "# Save as WAV file using scipy\n",
    "write(filename, samplerate, mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample audio\n",
    "For optimal results, we always use audio at 16kHz sample rate, mono, and wave type.\n",
    "\n",
    "⚠️You need **ffmep** installed on your system:\n",
    "https://github.com/BtbN/FFmpeg-Builds/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangwei/miniforge3/envs/SpeechRecognition/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def resample_audio(audio_path:str, output_path: str =\"resampled_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Resample an audio file to wav, 16khz, and mono\n",
    "\n",
    "    Returns: \n",
    "        - output_path\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    #audio = AudioSegment.from_wav(input_path)\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    \n",
    "    # Resample it\n",
    "    resampled_audio = audio.set_frame_rate(16000) # Vosk and Other models better works with 16kHz\n",
    "\n",
    "    # Convert the audio file to single channel (mono) (Vosk)\n",
    "    resampled_audio = resampled_audio.set_channels(1)\n",
    "    \n",
    "    # Export the resampled audio\n",
    "    resampled_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from ./models/vosk-model-fr-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from ./models/vosk-model-fr-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from ./models/vosk-model-fr-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo ./models/vosk-model-fr-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from ./models/vosk-model-fr-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from ./models/vosk-model-fr-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from ./models/vosk-model-fr-0.22/rnnlm/final.raw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "oui\n",
      "foncez\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hé\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bon j'ai débloqué des stocks\n",
      "mais ses cheveux étaient bonnes\n",
      "moi aussi mais c'est ses serveurs et son sang sera versé au son de gaza\n",
      "revenant sur la question\n",
      "femme à ce message peine toujours possible possède depuis l'espace issus des collections ferait bizarre tu remets ta colonne\n",
      "dombasle\n",
      "\n",
      "\n",
      "parfait\n",
      "passez votre serment\n",
      "d'autres fruit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bonjour\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "qui fonctionne pour te soutenir\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "bonjour\n",
      "a toi\n",
      "\n",
      "rien ne va plus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "neuf deux\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb Cellule 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m stream\u001b[39m.\u001b[39mstart_stream()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     data \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39;49mread(\u001b[39m4096\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# if len(data) == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m#     break\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m recognizer\u001b[39m.\u001b[39mAcceptWaveform(data):\n",
      "File \u001b[0;32m~/miniforge3/envs/SpeechRecognition/lib/python3.10/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mread_stream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream, num_frames,\n\u001b[1;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Live speech transcription\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "import pyaudio\n",
    "\n",
    "# model = Model(\"./vosk-model-fr-0.22\")\n",
    "model = Model(\"./models/vosk-model-fr-0.22\")\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "mic = pyaudio.PyAudio()\n",
    "stream = mic.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=8192)\n",
    "stream.start_stream()\n",
    "\n",
    "while True:\n",
    "    data = stream.read(4096)\n",
    "    # if len(data) == 0:\n",
    "    #     break\n",
    "    if recognizer.AcceptWaveform(data):\n",
    "        text = recognizer.Result()\n",
    "        # print(text)\n",
    "        print(text[14:-3])\n",
    "    # else:\n",
    "    #     print(recognizer.PartialResult())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Speech Recognition\n",
    "\n",
    "Testing the speech recognition with the google api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microphone with name \"Microphone MacBook Air\" found for `Microphone(device_index=0)`\n",
      "Microphone with name \"Haut-parleurs MacBook Air\" found for `Microphone(device_index=1)`\n",
      "Microphone with name \"Microphone de « Space Banan »\" found for `Microphone(device_index=2)`\n",
      "Microphone with name \"Microsoft Teams Audio\" found for `Microphone(device_index=3)`\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "    print(\"Microphone with name \\\"{1}\\\" found for `Microphone(device_index={0})`\".format(index, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my computer, my default microphone is Intel smart sound. It will be device_index=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microphone MacBook Air',\n",
       " 'Haut-parleurs MacBook Air',\n",
       " 'Microphone de «\\xa0Space Banan\\xa0»',\n",
       " 'Microsoft Teams Audio']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "sr.Microphone.list_microphone_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/python-speech-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak!\n",
      "End!\n"
     ]
    },
    {
     "ename": "UnknownValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb Cellule 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     audio_data \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mlisten(source)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnd!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m result \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mrecognize_google(audio_data, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfr-FR\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/SpeechRecognition/lib/python3.10/site-packages/speech_recognition/__init__.py:728\u001b[0m, in \u001b[0;36mRecognizer.recognize_google\u001b[0;34m(self, audio_data, key, language, pfilter, show_all, with_confidence)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m show_all:\n\u001b[1;32m    726\u001b[0m     \u001b[39mreturn\u001b[39;00m actual_result\n\u001b[0;32m--> 728\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(actual_result, \u001b[39mdict\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(actual_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39malternative\u001b[39m\u001b[39m\"\u001b[39m, [])) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mraise\u001b[39;00m UnknownValueError()\n\u001b[1;32m    730\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m actual_result[\u001b[39m\"\u001b[39m\u001b[39malternative\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    731\u001b[0m     \u001b[39m# return alternative with highest confidence score\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     best_hypothesis \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(actual_result[\u001b[39m\"\u001b[39m\u001b[39malternative\u001b[39m\u001b[39m\"\u001b[39m], key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m alternative: alternative[\u001b[39m\"\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mUnknownValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# micro = sr.Microphone(device_index=19)\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Load default microphone on the system\t\n",
    "micro = sr.Microphone()\n",
    "\n",
    "with micro as source:\n",
    "    print(\"Speak!\")\n",
    "    audio_data = r.listen(source)\n",
    "    print(\"End!\")\n",
    "result = r.recognize_google(audio_data, language=\"fr-FR\")\n",
    "print (\">\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak!\n",
      "End!\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "missing PocketSphinx language data directory: \"/Users/xiangwei/miniforge3/envs/SpeechRecognition/lib/python3.10/site-packages/speech_recognition/pocketsphinx-data/fr-FR\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb Cellule 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     audio_data \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mlisten(source)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnd!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m result \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mrecognize_sphinx(audio_data, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfr-FR\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/SpeechRecognition/lib/python3.10/site-packages/speech_recognition/__init__.py:612\u001b[0m, in \u001b[0;36mRecognizer.recognize_sphinx\u001b[0;34m(self, audio_data, language, keyword_entries, grammar, show_all)\u001b[0m\n\u001b[1;32m    610\u001b[0m language_directory \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrealpath(\u001b[39m__file__\u001b[39m)), \u001b[39m\"\u001b[39m\u001b[39mpocketsphinx-data\u001b[39m\u001b[39m\"\u001b[39m, language)\n\u001b[1;32m    611\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(language_directory):\n\u001b[0;32m--> 612\u001b[0m     \u001b[39mraise\u001b[39;00m RequestError(\u001b[39m\"\u001b[39m\u001b[39mmissing PocketSphinx language data directory: \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(language_directory))\n\u001b[1;32m    613\u001b[0m acoustic_parameters_directory \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(language_directory, \u001b[39m\"\u001b[39m\u001b[39macoustic-model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    614\u001b[0m language_model_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(language_directory, \u001b[39m\"\u001b[39m\u001b[39mlanguage-model.lm.bin\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRequestError\u001b[0m: missing PocketSphinx language data directory: \"/Users/xiangwei/miniforge3/envs/SpeechRecognition/lib/python3.10/site-packages/speech_recognition/pocketsphinx-data/fr-FR\""
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# micro = sr.Microphone(device_index=19)\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Load default microphone on the system\t\n",
    "micro = sr.Microphone()\n",
    "\n",
    "with micro as source:\n",
    "    print(\"Speak!\")\n",
    "    audio_data = r.listen(source)\n",
    "    print(\"End!\")\n",
    "result = r.recognize_sphinx(audio_data, language=\"fr-FR\")\n",
    "print (\">\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they do the use\n"
     ]
    }
   ],
   "source": [
    "from pocketsphinx import LiveSpeech\n",
    "for phrase in LiveSpeech(): print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the still smell of old beer lingers it takes heat to bring out the odour a cold dip restores health exist a salt pickle taste fine with him as well past or my favourite exist for food is the hot cross bun\n"
     ]
    }
   ],
   "source": [
    "harward = sr.AudioFile('./audio_files/harvard.wav')\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "with harward as source:\n",
    "    audio = r.record(source)\n",
    "result = r.recognize_google(audio)\n",
    "print(\">\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Speech is very powerfull but the licence is paid. So we can't use it for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep speech\n",
    "\n",
    "https://github.com/mozilla/DeepSpeech\n",
    "\n",
    "DeepSpeech by Mozilla is an open-source Speech-to-Text engine.\n",
    "\n",
    "+ Open source\n",
    "- Only English but adaptable (we will try)\n",
    "- After Mozilla restructure, the project is canceled.\n",
    "\n",
    "https://deepspeech.readthedocs.io/en/v0.9.3/USING.html#usage-docs\n",
    "\n",
    "We will use this community french:\n",
    "https://github.com/Common-Voice/commonvoice-fr/releases/tag/fr-v0.6\n",
    "\n",
    "https://discourse.mozilla.org/t/modele-francais-0-6-pour-deepspeech-v0-7-v0-8-v0-9/71993/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepspeech'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb Cellule 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeepspeech\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwave\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load the model and scorer\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepspeech'"
     ]
    }
   ],
   "source": [
    "import deepspeech\n",
    "import wave\n",
    "\n",
    "# Load the model and scorer\n",
    "model_path = './models/model_tensorflow_fr/output_graph.pbmm'\n",
    "scorer_path = './models/model_tensorflow_fr/kenlm.scorer'\n",
    "\n",
    "model = deepspeech.Model(model_path)\n",
    "model.enableExternalScorer(scorer_path)\n",
    "\n",
    "# Load an audio file (16-bit PCM WAV format)\n",
    "audio_file = './resampled_audio.wav'\n",
    "with open(audio_file, 'rb') as f:\n",
    "    audio_data = f.read()\n",
    "\n",
    "# Get the frame rate\n",
    "# import wave\n",
    "# with wave.open(audio_file, 'rb') as wf:\n",
    "#     # Ensure the audio is 16kHz, 16-bit PCM\n",
    "#     if wf.getframerate() != 16000:\n",
    "#         print(\"Warning: The audio sample rate is not 16kHz. You may need to resample for optimal results.\")\n",
    "    \n",
    "#     # Read the audio data\n",
    "#     audio_data = wf.readframes(wf.getnframes())\n",
    "\n",
    "with wave.open(audio_file, 'rb') as wf:\n",
    "    # ... other checks ...\n",
    "\n",
    "    # Read the audio data\n",
    "    audio_data = np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16)\n",
    "\n",
    "# Perform STT\n",
    "transcript = model.stt(audio_data)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, some test, their provided English model works very well with English but it does not perform well with French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaddleSpeech\n",
    "\n",
    "PaddleSpeech is an open-source toolkit on PaddlePaddle platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models.\n",
    "\n",
    "https://github.com/PaddlePaddle/PaddleSpeech\n",
    "\n",
    "https://paddlespeech.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddlespeech'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb Cellule 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpaddlespeech\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiangwei/Documents/GitHub/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m paddlespeech\u001b[39m.\u001b[39masr\u001b[39m.\u001b[39mload_model(\u001b[39m'\u001b[39m\u001b[39mpath_to_model\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpath_to_pretrained_weights\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paddlespeech'"
     ]
    }
   ],
   "source": [
    "import paddlespeech\n",
    "\n",
    "# Load the model\n",
    "model = paddlespeech.asr.load_model('path_to_model', 'path_to_pretrained_weights')\n",
    "\n",
    "# Process audio\n",
    "audio_data = paddlespeech.asr.preprocess('path_to_audio_file.wav')\n",
    "\n",
    "# Perform ASR\n",
    "transcription = model.transcribe(audio_data)\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'paddlespeech' has no attribute 's2t'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\loann\\Desktop\\Epitech\\AI\\T-AIA-901-LYO_1\\SpeechRecognition\\SpeechRecognition.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/loann/Desktop/Epitech/AI/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpaddlespeech\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/loann/Desktop/Epitech/AI/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Initialize the ASR model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/loann/Desktop/Epitech/AI/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m asr_model \u001b[39m=\u001b[39m paddlespeech\u001b[39m.\u001b[39;49ms2t\u001b[39m.\u001b[39mASRModel()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/loann/Desktop/Epitech/AI/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Transcribe an audio file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/loann/Desktop/Epitech/AI/T-AIA-901-LYO_1/SpeechRecognition/SpeechRecognition.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./audio_files/harvard.wav\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# replace with your audio file\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'paddlespeech' has no attribute 's2t'"
     ]
    }
   ],
   "source": [
    "import paddlespeech\n",
    "\n",
    "# Initialize the ASR model\n",
    "asr_model = paddlespeech.s2t.ASRModel()\n",
    "\n",
    "# Transcribe an audio file\n",
    "filename = \"./audio_files/harvard.wav\"  # replace with your audio file\n",
    "transcription = asr_model.transcribe_file(filename)\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vosk\n",
    "\n",
    "- Open-source\n",
    "- offline\n",
    "- supports 10+ language including french\n",
    "\n",
    "https://alphacephei.com/vosk\n",
    "https://alphacephei.com/vosk/models\n",
    "\n",
    "- vosk-model-fr-0.22 (Apache 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from ./models/vosk-model-fr-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from ./models/vosk-model-fr-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from ./models/vosk-model-fr-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo ./models/vosk-model-fr-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from ./models/vosk-model-fr-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from ./models/vosk-model-fr-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from ./models/vosk-model-fr-0.22/rnnlm/final.raw\n"
     ]
    }
   ],
   "source": [
    "from vosk import Model\n",
    "\n",
    "# Load the model\n",
    "model = Model(\"./models/vosk-model-fr-0.22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English']\n",
      "{\n",
      "  \"text\" : \"basse-terre smart small hot bird wenger est exquis de bruno peyron écran depp winstrol safe and ast et son décolleté femme tres tacos al pastor rarement fait vrai et c'est faux fdl s nigra cross bone\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "from vosk import KaldiRecognizer\n",
    "import wave\n",
    "\n",
    "# Prepare audio\n",
    "audio_file = \"./audio_files/harvard.wav\"\n",
    "audio_file = resample_audio(audio_file, \"./harvard.wav\")\n",
    "\n",
    "# Create a recognizer object\n",
    "rec = KaldiRecognizer(model, 16000)  # Replace with the sample rate of your audio\n",
    "\n",
    "\n",
    "# Process an audio file\n",
    "with wave.open(audio_file, \"rb\") as f:\n",
    "    while True:\n",
    "        data = f.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        rec.AcceptWaveform(data)\n",
    "\n",
    "# Print the final transcription\n",
    "print(rec.FinalResult())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Now we have a good model, we need to test it with lots of samples.\n",
    "\n",
    "We created some audios. OUr objective is to identify correctly the city A and the city B\n",
    "\n",
    "1) First we need to get how test audio data.\n",
    "\n",
    "Download these these files to zip:\n",
    "- https://drive.google.com/drive/folders/1ir7eqefODLuBq4Rw1rV7cJG1era-qzB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to ./audio_files/test\n",
      "Test folder data: ./audio_files/test\\voice_recognition_data\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# zip file name\n",
    "zip_file = \"voice_recognition_data-20230928T150022Z-001.zip\"\n",
    "\n",
    "# directory name where files will be extracted\n",
    "TEST_FOLDER = \"./audio_files/test\"\n",
    "\n",
    "# Create the directory where the files will be extracted\n",
    "os.makedirs(TEST_FOLDER, exist_ok=True)\n",
    "\n",
    "# Create a ZipFile object\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file into the directory\n",
    "    zip_ref.extractall(TEST_FOLDER)\n",
    "\n",
    "print(f\"Files extracted to {TEST_FOLDER}\")\n",
    "\n",
    "# update test DIR\n",
    "TEST_FOLDER = os.path.join(TEST_FOLDER, \"voice_recognition_data\")\n",
    "\n",
    "print(f\"Test folder data: {TEST_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now we can use ipytest to do Unit test in jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import json\n",
    "\n",
    "def transcript(audio_file) -> dict:\n",
    "    # Prepare audio\n",
    "    audio_file = resample_audio(audio_file, \"./resampled_audio.wav\")\n",
    "\n",
    "    # Create a recognizer object\n",
    "    rec = KaldiRecognizer(model, 16000)  # Replace with the sample rate of your audio\n",
    "\n",
    "    # Process an audio file\n",
    "    with wave.open(audio_file, \"rb\") as f:\n",
    "        while True:\n",
    "            data = f.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            rec.AcceptWaveform(data)\n",
    "\n",
    "    # Print the final transcription\n",
    "    return json.loads(rec.FinalResult())[\"text\"] # returns a JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pourriez-vous m'indiquer le chemin pour aller de paris à marseille s'il vous plaît\n",
      "['paris', 'marseille']\n",
      "je suis à lyon et je dois me rendre à toulouse quel est le meilleur itinéraire\n",
      "['lyon', 'toulouse']\n",
      "excusez-moi pouvez-vous me dire comment aller de bordeaux à nice\n",
      "['bordeaux', 'nice']\n",
      "je cherche à aller de nantes à strasbourg vous m'aider avec cette direction\n",
      "['nantes', 'strasbourg']\n",
      "quelle est la meilleure façon de se rendre de lille à montpellier en voiture\n",
      "['lille', 'montpellier']\n",
      "savez-vous comment je peux aller de grenoble à rennes\n",
      "['grenoble', 'rennes']\n",
      "je planifie un voyage de tours à nancy quelle route marocain m'entendez-vous\n",
      "['tours', 'nancy']\n",
      "je me demande quel est le trajet le plus rapide pour aller d'orléans à dijon joue des suggestions\n",
      "['orléans', 'dijon']\n",
      "pouvez-vous ajouter les directions pour aller de rouen à avignon\n",
      "['rouen', 'avignon']\n",
      "je souhaite voyager de brest à amiens quel chemin devrais-je prendre\n",
      "['brest', 'amiens']\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "DIRECTORY = os.path.join(TEST_FOLDER, \"audio_files\", \"IA_voice_1_a_10\")\n",
    "\n",
    "# read the excel\n",
    "audio_files_df = pd.read_excel(os.path.join(TEST_FOLDER, \"audio_files.xlsx\"))\n",
    "\n",
    "file_names = audio_files_df[\"file_name\"].values\n",
    "scripts = audio_files_df[\"script\"].values\n",
    "tags = audio_files_df[\"tags_to_recover\"].values\n",
    "\n",
    "# Converting each string into a list of cities\n",
    "city_lists = [tag.split(', ') for tag in tags]\n",
    "\n",
    "\n",
    "# Flattening the list of lists to get a single list of all cities\n",
    "all_cities = [city for sublist in city_lists for city in sublist]\n",
    "\n",
    "for file_name, tag in zip(file_names, city_lists):\n",
    "    # find audio file\n",
    "    audio_file = os.path.join(DIRECTORY, str(file_name))\n",
    "    if not os.path.exists(audio_file): \n",
    "        #print(f\"Non existant file: {audio_file}.\"); \n",
    "        continue;\n",
    "\n",
    "    # Transcript audio file\n",
    "    predicted_sentence = transcript(audio_file)\n",
    "\n",
    "    # Create a pattern that matches any city name in the list of all cities\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(all_cities) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    # Find all city names in the sentence\n",
    "    found_cities = re.findall(pattern, predicted_sentence)\n",
    "    \n",
    "    print(predicted_sentence)\n",
    "    print(found_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipytest\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "DIRECTORY = \"audio_files/test/voice_recognition_data/audio_files/\"\n",
    "\n",
    "audio_files = pd.read_excel(\"./audio_files.xlsx\")\n",
    "file_names = audio_files[\"file_name\"].values\n",
    "scripts = audio_files[\"script\"].values\n",
    "tags = audio_files[\"tags_to_recover\"].values\n",
    "\n",
    "# Converting each string into a list of cities\n",
    "city_lists = [tag.lower().split(', ') for tag in tags]\n",
    "\n",
    "# Lowercase all strings\n",
    "\n",
    "# Flattening the list of lists to get a single list of all cities\n",
    "all_cities = [city for sublist in city_lists for city in sublist]\n",
    "\n",
    "# Create a pattern that matches any city name in the list of all cities\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(all_cities) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Define the test case\n",
    "def test_speech_recognition():\n",
    "    for file_name, tag in zip(file_names, city_lists):\n",
    "        sentence = transcript(DIRECTORY + str(file_name) + \".m4a\")\n",
    "\n",
    "    \n",
    "        # Find all city names in the sentence\n",
    "        found_cities = re.findall(pattern, sentence)\n",
    "        assert found_cities == tag, f\"Expected {tag} !== {found_cities}\"\n",
    "\n",
    "# Run the tests\n",
    "ipytest.run('-qq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpeechBrain\n",
    "\n",
    "- Open-source\n",
    "- online\n",
    "- multiple language, trained on the VoxLingua107 Dataset\n",
    "\n",
    "- Website: https://speechbrain.github.io/\n",
    "- GitHub: https://github.com/speechbrain/speechbrain\n",
    "- HuggingFace: https://huggingface.co/speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mauvaise Langage\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/lang-id-commonlanguage_ecapa\", savedir=\"pretrained_models/lang-id-commonlanguage_ecapa\")\n",
    "\n",
    "out_prob, score, index, text_lab = classifier.classify_file('./audio_files/harvard.wav')\n",
    "if text_lab == ['French'] :\n",
    "    asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-commonvoice-fr\", savedir=\"pretrained_models/asr-crdnn-commonvoice-fr\")\n",
    "    asr_model.transcribe_file(\"./audio_files/harvard.wav\")\n",
    "else :\n",
    "    print('Mauvaise Langage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpeechRecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
