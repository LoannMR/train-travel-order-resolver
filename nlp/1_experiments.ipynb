{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing. Token classification\n",
    "\n",
    "For this work, we need to creates an AI capable of detecting in a French sentence a depature city and an arrival city.\n",
    "\n",
    "\"Comment aller de [ville A] a [ville B]\"\n",
    "\"Depuis [ville B], comment aller a [ville A]\"\n",
    "\n",
    "Manipulate texts is called a Natural Language Processing problem.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparations\n",
    "\n",
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>moment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Depuis La chaize-le-vicomte à La roche-sur-foron</td>\n",
       "      <td>La roche-sur-foron</td>\n",
       "      <td>La chaize-le-vicomte</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Itiniréraire jusqu'a Giromagny depuis Quimper</td>\n",
       "      <td>Giromagny</td>\n",
       "      <td>Quimper</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment aller de Boigneville à Longjumeau mardi</td>\n",
       "      <td>Longjumeau</td>\n",
       "      <td>Boigneville</td>\n",
       "      <td>mardi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je suis actuellement à Villers-sur-mer et j’ai...</td>\n",
       "      <td>Ferrières-en-bray</td>\n",
       "      <td>Villers-sur-mer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donne moi l'itinéraire pour aller à Fontenay-l...</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "      <td>Lizy-sur-ourcq</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>train Frontenex Domène</td>\n",
       "      <td>Domène</td>\n",
       "      <td>Frontenex</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Quel est l'itinéraire entre Castres et Étainhus</td>\n",
       "      <td>Étainhus</td>\n",
       "      <td>Castres</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Je pars de Laignes pour aller à Osséja</td>\n",
       "      <td>Osséja</td>\n",
       "      <td>Laignes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Je pars dimanche matin de Lapeyrouse pour alle...</td>\n",
       "      <td>Colombes</td>\n",
       "      <td>Lapeyrouse</td>\n",
       "      <td>dimanche matin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>J'aimerais me rendre sur Chevrières en partant...</td>\n",
       "      <td>Chevrières</td>\n",
       "      <td>Bruz</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                  to  \\\n",
       "0      Depuis La chaize-le-vicomte à La roche-sur-foron  La roche-sur-foron   \n",
       "1         Itiniréraire jusqu'a Giromagny depuis Quimper           Giromagny   \n",
       "2       Comment aller de Boigneville à Longjumeau mardi          Longjumeau   \n",
       "3     Je suis actuellement à Villers-sur-mer et j’ai...   Ferrières-en-bray   \n",
       "4     Donne moi l'itinéraire pour aller à Fontenay-l...  Fontenay-le-fleury   \n",
       "...                                                 ...                 ...   \n",
       "1995                             train Frontenex Domène              Domène   \n",
       "1996    Quel est l'itinéraire entre Castres et Étainhus            Étainhus   \n",
       "1997             Je pars de Laignes pour aller à Osséja              Osséja   \n",
       "1998  Je pars dimanche matin de Lapeyrouse pour alle...            Colombes   \n",
       "1999  J'aimerais me rendre sur Chevrières en partant...          Chevrières   \n",
       "\n",
       "                      from          moment  \n",
       "0     La chaize-le-vicomte             NaN  \n",
       "1                  Quimper             NaN  \n",
       "2              Boigneville           mardi  \n",
       "3          Villers-sur-mer             NaN  \n",
       "4           Lizy-sur-ourcq             NaN  \n",
       "...                    ...             ...  \n",
       "1995             Frontenex             NaN  \n",
       "1996               Castres             NaN  \n",
       "1997               Laignes             NaN  \n",
       "1998            Lapeyrouse  dimanche matin  \n",
       "1999                  Bruz             NaN  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./default_dataset.csv', delimiter=';', encoding='utf-8')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ville du pont',\n",
       " 'villers grelot',\n",
       " 'villars les blamont',\n",
       " 'les villedieu',\n",
       " 'villers buzon',\n",
       " 'villers la combe',\n",
       " 'villers sous chalamont',\n",
       " 'voujeaucourt',\n",
       " 'bouconville vauclair',\n",
       " 'bouresches']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_df = pd.read_csv(\"./data/cities.csv\", delimiter=\",\", encoding=\"utf-8\")\n",
    "cities = list(cities_df['label'])\n",
    "cities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7758"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.index('domene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>moment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Depuis La chaize-le-vicomte à La roche-sur-foron</td>\n",
       "      <td>La chaize-le-vicomte</td>\n",
       "      <td>La roche-sur-foron</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Itiniréraire jusqu'a Giromagny depuis Quimper</td>\n",
       "      <td>Quimper</td>\n",
       "      <td>Giromagny</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment aller de Boigneville à Longjumeau mardi</td>\n",
       "      <td>Boigneville</td>\n",
       "      <td>Longjumeau</td>\n",
       "      <td>mardi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je suis actuellement à Villers-sur-mer et j’ai...</td>\n",
       "      <td>Villers-sur-mer</td>\n",
       "      <td>Ferrières-en-bray</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donne moi l'itinéraire pour aller à Fontenay-l...</td>\n",
       "      <td>Lizy-sur-ourcq</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  from  \\\n",
       "0   Depuis La chaize-le-vicomte à La roche-sur-foron  La chaize-le-vicomte   \n",
       "1      Itiniréraire jusqu'a Giromagny depuis Quimper               Quimper   \n",
       "2    Comment aller de Boigneville à Longjumeau mardi           Boigneville   \n",
       "3  Je suis actuellement à Villers-sur-mer et j’ai...       Villers-sur-mer   \n",
       "4  Donne moi l'itinéraire pour aller à Fontenay-l...        Lizy-sur-ourcq   \n",
       "\n",
       "                   to moment  \n",
       "0  La roche-sur-foron    NaN  \n",
       "1           Giromagny    NaN  \n",
       "2          Longjumeau  mardi  \n",
       "3   Ferrières-en-bray    NaN  \n",
       "4  Fontenay-le-fleury    NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reindex(columns=['text', 'from', 'to', 'moment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Standardize capitalization\n",
    "def capitalize_cities(name):\n",
    "    return ' '.join(word.capitalize() for word in name.split())\n",
    "\n",
    "# if word.lower() not in ['de', 'la', 'le', 'sur', 'du', 'des', 'et'] else word\n",
    "\n",
    "# Manage (le ) Havre...\n",
    "def correct_city_name(city):\n",
    "    # Get all contents inside brackets\n",
    "    match = re.search(r\"\\((.*?)\\)\\s*$\", city)\n",
    "    if match:\n",
    "        content = match.group(1)  # Content separeted from bracket\n",
    "        city = city.replace(match.group(0), '').strip()  # Delete brackets\n",
    "        # Manage apostrophe\n",
    "        if content.endswith(\"'\"):\n",
    "            city = f\"{content.capitalize()}{city}\"  # no space\n",
    "        else:\n",
    "            city = f\"{content.capitalize()} {city}\"  # with space\n",
    "    return city\n",
    "\n",
    "df['from_corrected'] = df['from'].apply(correct_city_name).apply(capitalize_cities)\n",
    "df['to_corrected'] = df['to'].apply(correct_city_name).apply(capitalize_cities)\n",
    "\n",
    "#correct_city_name('Pavillons-sous-bois (les)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Depuis la chaize-le-vicomte à la roche-sur-foron\n",
       "1           Itiniréraire jusqu'a giromagny depuis quimper\n",
       "2         Comment aller de boigneville à longjumeau mardi\n",
       "3       Je suis actuellement à villers-sur-mer et j’ai...\n",
       "4       Donne moi l'itinéraire pour aller à fontenay-l...\n",
       "                              ...                        \n",
       "1995                               Train frontenex domène\n",
       "1996      Quel est l'itinéraire entre castres et étainhus\n",
       "1997               Je pars de laignes pour aller à osséja\n",
       "1998    Je pars dimanche matin de lapeyrouse pour alle...\n",
       "1999    J'aimerais me rendre sur chevrières en partant...\n",
       "Name: text, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_text(row):\n",
    "    text = row['text']\n",
    "    from_city = row['from']\n",
    "    to_city = row['to']\n",
    "    from_city_capitalized = row['from_corrected']\n",
    "    to_city_capitalized = row['to_corrected']\n",
    "\n",
    "    # Replaces city names to their capitalized version\n",
    "    text = text.replace(from_city, from_city_capitalized).replace(to_city, to_city_capitalized)\n",
    "    return text\n",
    "\n",
    "# Apply the function to each rows\n",
    "df['text'] = df.apply(update_text, axis=1)\n",
    "\n",
    "# capitalize each text\n",
    "df['text'].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>moment</th>\n",
       "      <th>from_corrected</th>\n",
       "      <th>to_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Depuis La Chaize-le-vicomte à La Roche-sur-foron</td>\n",
       "      <td>La chaize-le-vicomte</td>\n",
       "      <td>La roche-sur-foron</td>\n",
       "      <td>NaN</td>\n",
       "      <td>La Chaize-le-vicomte</td>\n",
       "      <td>La Roche-sur-foron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Itiniréraire jusqu'a Giromagny depuis Quimper</td>\n",
       "      <td>Quimper</td>\n",
       "      <td>Giromagny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quimper</td>\n",
       "      <td>Giromagny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment aller de Boigneville à Longjumeau mardi</td>\n",
       "      <td>Boigneville</td>\n",
       "      <td>Longjumeau</td>\n",
       "      <td>mardi</td>\n",
       "      <td>Boigneville</td>\n",
       "      <td>Longjumeau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je suis actuellement à Villers-sur-mer et j’ai...</td>\n",
       "      <td>Villers-sur-mer</td>\n",
       "      <td>Ferrières-en-bray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Villers-sur-mer</td>\n",
       "      <td>Ferrières-en-bray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donne moi l'itinéraire pour aller à Fontenay-l...</td>\n",
       "      <td>Lizy-sur-ourcq</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lizy-sur-ourcq</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  from  \\\n",
       "0   Depuis La Chaize-le-vicomte à La Roche-sur-foron  La chaize-le-vicomte   \n",
       "1      Itiniréraire jusqu'a Giromagny depuis Quimper               Quimper   \n",
       "2    Comment aller de Boigneville à Longjumeau mardi           Boigneville   \n",
       "3  Je suis actuellement à Villers-sur-mer et j’ai...       Villers-sur-mer   \n",
       "4  Donne moi l'itinéraire pour aller à Fontenay-l...        Lizy-sur-ourcq   \n",
       "\n",
       "                   to moment        from_corrected        to_corrected  \n",
       "0  La roche-sur-foron    NaN  La Chaize-le-vicomte  La Roche-sur-foron  \n",
       "1           Giromagny    NaN               Quimper           Giromagny  \n",
       "2          Longjumeau  mardi           Boigneville          Longjumeau  \n",
       "3   Ferrières-en-bray    NaN       Villers-sur-mer   Ferrières-en-bray  \n",
       "4  Fontenay-le-fleury    NaN        Lizy-sur-ourcq  Fontenay-le-fleury  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>moment</th>\n",
       "      <th>from_corrected</th>\n",
       "      <th>to_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, from, to, moment, from_corrected, to_corrected]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select lines which contains brackets\n",
    "df[df['text'].str.contains(r'\\(.*\\)', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>moment</th>\n",
       "      <th>from_corrected</th>\n",
       "      <th>to_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, from, to, moment, from_corrected, to_corrected]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].str.contains(\"Aigle\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessarry columns\n",
    "df = df.drop(['from', 'to', 'moment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Depuis La Chaize-le-vicomte à La Roche-sur-foron</td>\n",
       "      <td>La Chaize-le-vicomte</td>\n",
       "      <td>La Roche-sur-foron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Itiniréraire jusqu'a Giromagny depuis Quimper</td>\n",
       "      <td>Quimper</td>\n",
       "      <td>Giromagny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment aller de Boigneville à Longjumeau mardi</td>\n",
       "      <td>Boigneville</td>\n",
       "      <td>Longjumeau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je suis actuellement à Villers-sur-mer et j’ai...</td>\n",
       "      <td>Villers-sur-mer</td>\n",
       "      <td>Ferrières-en-bray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donne moi l'itinéraire pour aller à Fontenay-l...</td>\n",
       "      <td>Lizy-sur-ourcq</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>train Frontenex Domène</td>\n",
       "      <td>Frontenex</td>\n",
       "      <td>Domène</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Quel est l'itinéraire entre Castres et Étainhus</td>\n",
       "      <td>Castres</td>\n",
       "      <td>Étainhus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Je pars de Laignes pour aller à Osséja</td>\n",
       "      <td>Laignes</td>\n",
       "      <td>Osséja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Je pars dimanche matin de Lapeyrouse pour alle...</td>\n",
       "      <td>Lapeyrouse</td>\n",
       "      <td>Colombes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>J'aimerais me rendre sur Chevrières en partant...</td>\n",
       "      <td>Bruz</td>\n",
       "      <td>Chevrières</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                  from  \\\n",
       "0      Depuis La Chaize-le-vicomte à La Roche-sur-foron  La Chaize-le-vicomte   \n",
       "1         Itiniréraire jusqu'a Giromagny depuis Quimper               Quimper   \n",
       "2       Comment aller de Boigneville à Longjumeau mardi           Boigneville   \n",
       "3     Je suis actuellement à Villers-sur-mer et j’ai...       Villers-sur-mer   \n",
       "4     Donne moi l'itinéraire pour aller à Fontenay-l...        Lizy-sur-ourcq   \n",
       "...                                                 ...                   ...   \n",
       "1995                             train Frontenex Domène             Frontenex   \n",
       "1996    Quel est l'itinéraire entre Castres et Étainhus               Castres   \n",
       "1997             Je pars de Laignes pour aller à Osséja               Laignes   \n",
       "1998  Je pars dimanche matin de Lapeyrouse pour alle...            Lapeyrouse   \n",
       "1999  J'aimerais me rendre sur Chevrières en partant...                  Bruz   \n",
       "\n",
       "                      to  \n",
       "0     La Roche-sur-foron  \n",
       "1              Giromagny  \n",
       "2             Longjumeau  \n",
       "3      Ferrières-en-bray  \n",
       "4     Fontenay-le-fleury  \n",
       "...                  ...  \n",
       "1995              Domène  \n",
       "1996            Étainhus  \n",
       "1997              Osséja  \n",
       "1998            Colombes  \n",
       "1999          Chevrières  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={'from_corrected': 'from', 'to_corrected': 'to'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('cleaned_dataframe.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelized Data\n",
    "\n",
    "This problem is a NER problem. Our model's goal is to classify each word to be a city **FROM** or **TO**.\n",
    "\n",
    "Our cities contains multiple word like \"La Chaize-le-vicomte\" we need o process them.\n",
    "We can process them using IOB tagging system.\n",
    "\n",
    "In our case, it will be:\n",
    "-  **B**: beginning of the chunk\n",
    "- **I**: inside of the chunk\n",
    "- **FROM**: city to departure\n",
    "- **TO**: city to arrive\n",
    "- **O**: Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>IOB_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Depuis La Chaize-le-vicomte à La Roche-sur-foron</td>\n",
       "      <td>La Chaize-le-vicomte</td>\n",
       "      <td>La Roche-sur-foron</td>\n",
       "      <td>[O, B-FROM, I-FROM, O, B-TO, I-TO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Itiniréraire jusqu'a Giromagny depuis Quimper</td>\n",
       "      <td>Quimper</td>\n",
       "      <td>Giromagny</td>\n",
       "      <td>[O, O, B-TO, O, B-FROM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment aller de Boigneville à Longjumeau mardi</td>\n",
       "      <td>Boigneville</td>\n",
       "      <td>Longjumeau</td>\n",
       "      <td>[O, O, O, B-FROM, O, B-TO, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je suis actuellement à Villers-sur-mer et j’ai...</td>\n",
       "      <td>Villers-sur-mer</td>\n",
       "      <td>Ferrières-en-bray</td>\n",
       "      <td>[O, O, O, O, B-FROM, O, O, O, O, B-TO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donne moi l'itinéraire pour aller à Fontenay-l...</td>\n",
       "      <td>Lizy-sur-ourcq</td>\n",
       "      <td>Fontenay-le-fleury</td>\n",
       "      <td>[O, O, O, O, O, O, B-TO, O, O, O, B-FROM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>train Frontenex Domène</td>\n",
       "      <td>Frontenex</td>\n",
       "      <td>Domène</td>\n",
       "      <td>[O, B-FROM, B-TO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Quel est l'itinéraire entre Castres et Étainhus</td>\n",
       "      <td>Castres</td>\n",
       "      <td>Étainhus</td>\n",
       "      <td>[O, O, O, O, B-FROM, O, B-TO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Je pars de Laignes pour aller à Osséja</td>\n",
       "      <td>Laignes</td>\n",
       "      <td>Osséja</td>\n",
       "      <td>[O, O, O, B-FROM, O, O, O, B-TO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Je pars dimanche matin de Lapeyrouse pour alle...</td>\n",
       "      <td>Lapeyrouse</td>\n",
       "      <td>Colombes</td>\n",
       "      <td>[O, O, O, O, O, B-FROM, O, O, O, B-TO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>J'aimerais me rendre sur Chevrières en partant...</td>\n",
       "      <td>Bruz</td>\n",
       "      <td>Chevrières</td>\n",
       "      <td>[O, O, O, O, B-TO, O, O, O, B-FROM]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                  from  \\\n",
       "0      Depuis La Chaize-le-vicomte à La Roche-sur-foron  La Chaize-le-vicomte   \n",
       "1         Itiniréraire jusqu'a Giromagny depuis Quimper               Quimper   \n",
       "2       Comment aller de Boigneville à Longjumeau mardi           Boigneville   \n",
       "3     Je suis actuellement à Villers-sur-mer et j’ai...       Villers-sur-mer   \n",
       "4     Donne moi l'itinéraire pour aller à Fontenay-l...        Lizy-sur-ourcq   \n",
       "...                                                 ...                   ...   \n",
       "1995                             train Frontenex Domène             Frontenex   \n",
       "1996    Quel est l'itinéraire entre Castres et Étainhus               Castres   \n",
       "1997             Je pars de Laignes pour aller à Osséja               Laignes   \n",
       "1998  Je pars dimanche matin de Lapeyrouse pour alle...            Lapeyrouse   \n",
       "1999  J'aimerais me rendre sur Chevrières en partant...                  Bruz   \n",
       "\n",
       "                      to                                 IOB_labels  \n",
       "0     La Roche-sur-foron         [O, B-FROM, I-FROM, O, B-TO, I-TO]  \n",
       "1              Giromagny                    [O, O, B-TO, O, B-FROM]  \n",
       "2             Longjumeau              [O, O, O, B-FROM, O, B-TO, O]  \n",
       "3      Ferrières-en-bray     [O, O, O, O, B-FROM, O, O, O, O, B-TO]  \n",
       "4     Fontenay-le-fleury  [O, O, O, O, O, O, B-TO, O, O, O, B-FROM]  \n",
       "...                  ...                                        ...  \n",
       "1995              Domène                          [O, B-FROM, B-TO]  \n",
       "1996            Étainhus              [O, O, O, O, B-FROM, O, B-TO]  \n",
       "1997              Osséja           [O, O, O, B-FROM, O, O, O, B-TO]  \n",
       "1998            Colombes     [O, O, O, O, O, B-FROM, O, O, O, B-TO]  \n",
       "1999          Chevrières        [O, O, O, O, B-TO, O, O, O, B-FROM]  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create labels sequence using IOB system (for multi word city names)\n",
    "def create_IOB_label_sequence(sentence, city_from, city_to):\n",
    "    words = sentence.split()  \n",
    "    labels = []  \n",
    "\n",
    "    city_from_words = city_from.split()\n",
    "    city_to_words = city_to.split()\n",
    "\n",
    "    index = 0  \n",
    "    while index < len(words):\n",
    "        if ' '.join(words[index:index+len(city_from_words)]) == city_from:\n",
    "            labels.append('B-FROM')\n",
    "            labels.extend(['I-FROM'] * (len(city_from_words) - 1))\n",
    "            index += len(city_from_words)\n",
    "        elif ' '.join(words[index:index+len(city_to_words)]) == city_to:\n",
    "            labels.append('B-TO')\n",
    "            labels.extend(['I-TO'] * (len(city_to_words) - 1))\n",
    "            index += len(city_to_words)\n",
    "        else:\n",
    "            labels.append('O')\n",
    "            index += 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "df['IOB_labels'] = df.apply(lambda row: create_IOB_label_sequence(row['text'], row['from'], row['to']), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Depuis La Chaize-le-vicomte à La Roche-sur-foron',\n",
       "        \"Itiniréraire jusqu'a Giromagny depuis Quimper\",\n",
       "        'Comment aller de Boigneville à Longjumeau mardi',\n",
       "        'Je suis actuellement à Villers-sur-mer et j’aimerais partir à Ferrières-en-bray',\n",
       "        \"Donne moi l'itinéraire pour aller à Fontenay-le-fleury en partant de Lizy-sur-ourcq\",\n",
       "        'Comment aller de Viviers-du-lac à Aytré',\n",
       "        'Je pars de Caen pour aller à Villeneuve-la-comtesse',\n",
       "        \"le départ et l'arrivée de mon voyage sont Sathonay-camp et Quillan\",\n",
       "        'Quel trajet pour Éveux Grandvilliers',\n",
       "        'train Wizernes Sillé-le-guillaume'], dtype=object),\n",
       " array([list(['O', 'B-FROM', 'I-FROM', 'O', 'B-TO', 'I-TO']),\n",
       "        list(['O', 'O', 'B-TO', 'O', 'B-FROM']),\n",
       "        list(['O', 'O', 'O', 'B-FROM', 'O', 'B-TO', 'O']),\n",
       "        list(['O', 'O', 'O', 'O', 'B-FROM', 'O', 'O', 'O', 'O', 'B-TO']),\n",
       "        list(['O', 'O', 'O', 'O', 'O', 'O', 'B-TO', 'O', 'O', 'O', 'B-FROM']),\n",
       "        list(['O', 'O', 'O', 'B-FROM', 'O', 'B-TO']),\n",
       "        list(['O', 'O', 'O', 'B-FROM', 'O', 'O', 'O', 'B-TO']),\n",
       "        list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FROM', 'O', 'B-TO']),\n",
       "        list(['O', 'O', 'O', 'B-FROM', 'B-TO']),\n",
       "        list(['O', 'B-FROM', 'B-TO'])], dtype=object))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['text'].values\n",
    "y = df['IOB_labels'].values\n",
    "\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Je prends le train Saint-mard Puy-guillaume',\n",
       "        \"j'aimerais partir de Reignier pour me rendre à Digoin\",\n",
       "        'trajet Vertou Leyment',\n",
       "        'je viens de Pontorson et je veux aller à Loudun',\n",
       "        'Je suis à Hatrize et je souhaite me rendre à Vonnas'],\n",
       "       dtype=object),\n",
       " array([list(['O', 'O', 'O', 'O', 'B-FROM', 'B-TO']),\n",
       "        list(['O', 'O', 'O', 'B-FROM', 'O', 'O', 'O', 'O', 'B-TO']),\n",
       "        list(['O', 'B-FROM', 'B-TO']),\n",
       "        list(['O', 'O', 'O', 'B-FROM', 'O', 'O', 'O', 'O', 'O', 'B-TO']),\n",
       "        list(['O', 'O', 'O', 'B-FROM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TO'])],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text vectorization (Tokenisation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing X (Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "\n",
    "max_vocab_length = 3000 # max number of words to have in our vocabulary \n",
    "max_length = 30 # max length our sequences will be\n",
    "\n",
    "# Custom standardization function\n",
    "def custom_standardization(input_text):\n",
    "    # Remove punctuations, but preserve apostrophes\n",
    "    return tf.strings.regex_replace(input_text, \"[^a-zA-Z0-9À-ÖØ-öø-ÿ' ]\", \"\")\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length, # max number of words to have in our vocabulary\n",
    "                                    standardize=custom_standardization,\n",
    "                                    split=\"whitespace\",\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length\n",
    "                                    )\n",
    "\n",
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the longest sentence by word count\n",
    "longest_sentence = df['text'].explode().apply(lambda x: len(x.split())).idxmax()\n",
    "longest_sentence_text = df['text'].explode()[longest_sentence]\n",
    "\n",
    "len(longest_sentence_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " je vais à Us depuis Égletons       \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 30), dtype=int64, numpy=\n",
       "array([[ 10,  55,   2, 257,  11, 233,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "import random\n",
    "\n",
    "random_sentence = random.choice(X_train)\n",
    "print(f\"Original text:\\n {random_sentence} \\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 2206\n",
      "5 most common words: ['', '[UNK]', 'à', 'de', 'Je', 'aller', 'pour']\n",
      "5 least common words: ['Aillysurnoye', 'Aiguesmortes', 'Ahun', 'Agde', 'Acheres']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary() # get all of the unique words in our training data\n",
    "top_5_words = words_in_vocab[:5+2] # get most common words (with the 2 specials characters, O: padding token, UNK: unknown)\n",
    "bottom_5_words = words_in_vocab[-5:] # get the least common words\n",
    "\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"5 most common words: {top_5_words}\")\n",
    "print(f\"5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Y (IOB Labels)\n",
    "\n",
    "IOB labels should be converted also to numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15164,), dtype=string, numpy=\n",
       "array([b'O', b'B-FROM', b'I-FROM', ..., b'O', b'O', b'B-FROM'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# flatten labels\n",
    "y_flatten = list(itertools.chain.from_iterable(y))\n",
    "y_flatten_tensor = tf.convert_to_tensor(y_flatten)\n",
    "y_flatten_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-TO', 'B-FROM', 'I-TO', 'I-FROM']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import StringLookup\n",
    "\n",
    "label_lookup = StringLookup(output_mode='int',\n",
    "                            max_tokens=5,\n",
    "                            num_oov_indices=0)\n",
    "\n",
    "# Fit the layer on training labels\n",
    "label_lookup.adapt(y_flatten_tensor)\n",
    "\n",
    "# Transform labels to integers\n",
    "label_lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0, 0, 0, 0, 2, 1], dtype=int64),\n",
       "  array([0, 0, 0, 2, 0, 0, 0, 0, 1], dtype=int64),\n",
       "  array([0, 2, 1], dtype=int64),\n",
       "  array([0, 0, 0, 2, 0, 0, 0, 0, 0, 1], dtype=int64),\n",
       "  array([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1], dtype=int64)],\n",
       " [array([0, 0, 0, 2, 0, 1], dtype=int64),\n",
       "  array([0, 0, 0, 0, 2, 0, 1], dtype=int64),\n",
       "  array([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1], dtype=int64),\n",
       "  array([0, 0, 0, 0, 1, 0, 0], dtype=int64),\n",
       "  array([0, 0, 0, 1, 0, 2], dtype=int64)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_transformed = [label_lookup(labels).numpy() for labels in y_train]\n",
    "y_test_transformed = [label_lookup(labels).numpy() for labels in y_test]\n",
    "\n",
    "y_train_transformed[:5], y_test_transformed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 2, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 0, 2, 0, 0, 0, 0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 2, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5]]),\n",
       " array([[0, 0, 0, 2, 0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 0, 0, 2, 0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 0, 1, 0, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 5, 5]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad my y to be equals at X padded size\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define a padding value that's different from any actual IOB tags\n",
    "PADDING_LABEL = label_lookup.vocabulary_size()  # 'num_tags' is the number of IOB tags you have\n",
    "\n",
    "y_train_padded = pad_sequences(y_train_transformed, maxlen=max_length, padding='post', value=PADDING_LABEL)\n",
    "y_test_padded = pad_sequences(y_test_transformed, maxlen=max_length, padding='post', value=PADDING_LABEL)\n",
    "\n",
    "y_train_padded[:5], y_test_padded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a modelling checkpoint callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel(logging.WARNING) # remove INFO log (model saved at...)\n",
    "\n",
    "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
    "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(save_path, model_name),\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0, # only output a limited amount of text\n",
    "        save_best_only=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\loannmr\\AppData\\Local\\Temp\\ipykernel_16588\\2512839750.py\", line 20, in <module>\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1783, in fit\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\backend.py\", line 5777, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [32,6] and labels shape [960]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_1549076]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[203], line 20\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      9\u001b[0m     layers\u001b[38;5;241m.\u001b[39mInput((\u001b[38;5;241m1\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring),\n\u001b[0;32m     10\u001b[0m     text_vectorizer, \u001b[38;5;66;03m# turn the input text into numbers\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(num_tags, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     14\u001b[0m ], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[0;32m     17\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[0;32m     18\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train,\n\u001b[0;32m     21\u001b[0m           y_train_padded,\n\u001b[0;32m     22\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     23\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test_padded))\n",
      "File \u001b[1;32mc:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\loannmr\\AppData\\Local\\Temp\\ipykernel_16588\\2512839750.py\", line 20, in <module>\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1783, in fit\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"c:\\Users\\loannmr\\.conda\\envs\\AI\\Lib\\site-packages\\keras\\src\\backend.py\", line 5777, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [32,6] and labels shape [960]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_1549076]"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "max_vocab_length = text_vectorizer.vocabulary_size() # Max number of words in the vocabulary\n",
    "max_length = max_length # Max length of each sequence\n",
    "num_tags = label_lookup.vocabulary_size() + 1 # Number of unique IOB tags (including 'O' and PADDING_LABEL)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input((1,), dtype=tf.string),\n",
    "    text_vectorizer, # turn the input text into numbers\n",
    "    layers.Embedding(input_dim=max_vocab_length, output_dim=128),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(num_tags, activation=\"relu\"),\n",
    "], name=\"model_1\")\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train_padded,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 9s 105ms/step - loss: 1.2569 - accuracy: 0.6999 - val_loss: 0.7262 - val_accuracy: 0.7265\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.4441 - accuracy: 0.8325 - val_loss: 0.2191 - val_accuracy: 0.9344\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.1588 - accuracy: 0.9516 - val_loss: 0.1144 - val_accuracy: 0.9620\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 5s 89ms/step - loss: 0.0934 - accuracy: 0.9672 - val_loss: 0.0813 - val_accuracy: 0.9710\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.0653 - accuracy: 0.9816 - val_loss: 0.0640 - val_accuracy: 0.9781\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 5s 90ms/step - loss: 0.0479 - accuracy: 0.9874 - val_loss: 0.0518 - val_accuracy: 0.9916\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.0356 - accuracy: 0.9927 - val_loss: 0.0423 - val_accuracy: 0.9923\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.0250 - accuracy: 0.9952 - val_loss: 0.0371 - val_accuracy: 0.9929\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 5s 87ms/step - loss: 0.0175 - accuracy: 0.9964 - val_loss: 0.0317 - val_accuracy: 0.9936\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.0128 - accuracy: 0.9986 - val_loss: 0.0299 - val_accuracy: 0.9936\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 5s 88ms/step - loss: 0.0097 - accuracy: 0.9993 - val_loss: 0.0279 - val_accuracy: 0.9936\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 5s 86ms/step - loss: 0.0070 - accuracy: 0.9993 - val_loss: 0.0273 - val_accuracy: 0.9936\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 5s 86ms/step - loss: 0.0054 - accuracy: 0.9996 - val_loss: 0.0264 - val_accuracy: 0.9936\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 11s 195ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 0.0253 - val_accuracy: 0.9936\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.0256 - val_accuracy: 0.9936\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 5s 84ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0251 - val_accuracy: 0.9936\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 5s 92ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0239 - val_accuracy: 0.9936\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0239 - val_accuracy: 0.9936\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.0239 - val_accuracy: 0.9936\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.0245 - val_accuracy: 0.9936\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 6s 106ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.0229 - val_accuracy: 0.9936\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.0230 - val_accuracy: 0.9929\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 6s 107ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0223 - val_accuracy: 0.9936\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0226 - val_accuracy: 0.9936\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0226 - val_accuracy: 0.9929\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 6s 105ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0223 - val_accuracy: 0.9936\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0223 - val_accuracy: 0.9936\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 6s 106ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0219 - val_accuracy: 0.9936\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0220 - val_accuracy: 0.9936\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 5s 97ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0219 - val_accuracy: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b5e06e0490>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "max_vocab_length = text_vectorizer.vocabulary_size() + 1 # Max number of words in the vocabulary (+ OOV token)\n",
    "max_length = max_length # Max length of each sequence\n",
    "num_tags = label_lookup.vocabulary_size() + 1 # Number of unique IOB tags (including 'O' and PADDING_LABEL)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input((1,), dtype=tf.string), # raw input string\n",
    "    text_vectorizer, # turn the input text into numbers\n",
    "    layers.Embedding(input_dim=max_vocab_length + 1, output_dim=128, mask_zero=True), # tells the model to ignore padded 0 (enhance correct sentences but more computation)\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.TimeDistributed(layers.Dense(num_tags, activation=\"softmax\")), # make a prediction for each word in the sequence.\n",
    "], name=\"model_1\")\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train_padded,\n",
    "          epochs=30,\n",
    "          validation_data=(X_test, y_test_padded),\n",
    "          verbose=1,\n",
    "          callbacks=[create_model_checkpoint(model_name=model.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.021895453333854675, 0.993565022945404]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss , accuracy\n",
    "model.evaluate(X_test, y_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 7ms/step - loss: 0.0219 - accuracy: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.021895453333854675, 0.993565022945404]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "model_1 = keras.models.load_model('model_experiments/model_1', custom_objects={'custom_standardization': custom_standardization})\n",
    "model_1.evaluate(X_test, y_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 30), dtype=int64, numpy=\n",
       "array([[105,  65, 307,   1,   1,   1,   2,   1,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = [\"Itinéraire vers Rouen dès que j'arrive à Paris\"]\n",
    "#sample_text = [\"Je veux aller de Lyon à Paris\"]\n",
    "\n",
    "text_vectorizer(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predictions = model_1.predict(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-TO', 'B-FROM', 'I-TO', 'I-FROM']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       "array([0, 0, 2, 1, 1, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(predictions[0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(sentences):\n",
    "    decoded_predictions = []\n",
    "\n",
    "    # make predictions\n",
    "    predictions = model_1.predict(sentences)\n",
    "\n",
    "    # mask padding and decode prediction\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        actual_length = len(sentences[i].split())  # Length of the actual sentence\n",
    "        \n",
    "        predicted_tags = tf.argmax(prediction, axis=-1).numpy()[:actual_length]  # Consider only actual length\n",
    "        # Map predicted tags to their labels, ignoring padding\n",
    "        decoded_predictions.append([label_lookup.get_vocabulary()[tag] for tag in predicted_tags])\n",
    "\n",
    "    return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n",
      "Itinéraire vers Rouen dès que j'arrive à Paris\n",
      "['O', 'O', 'B-FROM', 'B-TO', 'B-TO', 'I-TO', 'O', 'B-FROM']\n",
      "Je suis à Paris et je souhaite me rendre à Lyon\n",
      "['O', 'O', 'O', 'B-FROM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TO']\n",
      "Comment partir à Lyon depuis Toulouse\n",
      "['O', 'O', 'O', 'B-TO', 'O', 'B-FROM']\n",
      "Je mange des baguette\n",
      "['O', 'B-FROM', 'O', 'B-TO']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Itinéraire vers Rouen dès que j'arrive à Paris\",\n",
    "    \"Je suis à Paris et je souhaite me rendre à Lyon\",\n",
    "    \"Comment partir à Lyon depuis Toulouse\",\n",
    "    \"Je mange des baguette\"\n",
    "]\n",
    "\n",
    "predictions = make_prediction(sentences)\n",
    "\n",
    "\n",
    "for sentence, prediction in zip(sentences, predictions):\n",
    "    print(sentence)\n",
    "    print(prediction)\n",
    "\n",
    "# #make_prediction([])\n",
    "# #make_prediction([])\n",
    "# make_prediction([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que des phrases aléatoires, notre model est perdu. Il faut ameliorer notre jeu de donnee, en rajoutant des phrases avec 0 villes ou 1 villes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
